{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d50257",
   "metadata": {},
   "source": [
    "# Analysis of Transformer-based Models\n",
    "\n",
    "This notebook analyzes the sentiment analysis results from pre-trained transformer models (BERT, RoBERTa, ALBERT, XLNet) on both the gold and premo datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a6092a",
   "metadata": {},
   "source": [
    "Loads the libraries necessary for the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12feaa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec8b81",
   "metadata": {},
   "source": [
    "## Gold Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6097b5",
   "metadata": {},
   "source": [
    "Load results from transformer models on the gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9dfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"bert\", \"roberta\", \"albert\", \"xlnet\"]\n",
    "\n",
    "gold_results = {}\n",
    "for model in MODELS:\n",
    "    filepath = Path(f\"output_transformers/{model}_gold.json\")\n",
    "    if filepath.exists():\n",
    "        with open(filepath) as f:\n",
    "            gold_results[model] = json.load(f)\n",
    "    else:\n",
    "        print(f\"Missing file: {filepath}\")\n",
    "\n",
    "print(\"Loaded gold dataset results for models:\", list(gold_results.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6f740",
   "metadata": {},
   "source": [
    "Generate LaTeX Table for Gold Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_models = sorted(gold_results.keys())\n",
    "\n",
    "all_metrics = {}\n",
    "for model in sorted_models:\n",
    "    data = gold_results[model]\n",
    "    metrics = data['metrics']\n",
    "    \n",
    "    all_metrics[model] = {\n",
    "        'precision': [\n",
    "            metrics['per_class']['precision']['positive'],\n",
    "            metrics['per_class']['precision']['negative'],\n",
    "            metrics['per_class']['precision']['neutral']\n",
    "        ],\n",
    "        'recall': [\n",
    "            metrics['per_class']['recall']['positive'],\n",
    "            metrics['per_class']['recall']['negative'],\n",
    "            metrics['per_class']['recall']['neutral']\n",
    "        ],\n",
    "        'f1': [\n",
    "            metrics['per_class']['f1_score']['positive'],\n",
    "            metrics['per_class']['f1_score']['negative'],\n",
    "            metrics['per_class']['f1_score']['neutral']\n",
    "        ],\n",
    "        'macro_precision': metrics['macro']['precision'],\n",
    "        'micro_precision': metrics['micro']['precision'],\n",
    "        'macro_recall': metrics['macro']['recall'],\n",
    "        'micro_recall': metrics['micro']['recall'],\n",
    "        'macro_f1': metrics['macro']['f1_score'],\n",
    "        'micro_f1': metrics['micro']['f1_score'],\n",
    "        'n': data.get('n', 'N/A')\n",
    "    }\n",
    "\n",
    "best_values = {\n",
    "    'pos_prec': max(m['precision'][0] for m in all_metrics.values()),\n",
    "    'pos_rec': max(m['recall'][0] for m in all_metrics.values()),\n",
    "    'pos_f1': max(m['f1'][0] for m in all_metrics.values()),\n",
    "    'neg_prec': max(m['precision'][1] for m in all_metrics.values()),\n",
    "    'neg_rec': max(m['recall'][1] for m in all_metrics.values()),\n",
    "    'neg_f1': max(m['f1'][1] for m in all_metrics.values()),\n",
    "    'neu_prec': max(m['precision'][2] for m in all_metrics.values()),\n",
    "    'neu_rec': max(m['recall'][2] for m in all_metrics.values()),\n",
    "    'neu_f1': max(m['f1'][2] for m in all_metrics.values()),\n",
    "    'macro_prec': max(m['macro_precision'] for m in all_metrics.values()),\n",
    "    'macro_rec': max(m['macro_recall'] for m in all_metrics.values()),\n",
    "    'macro_f1': max(m['macro_f1'] for m in all_metrics.values()),\n",
    "    'micro_prec': max(m['micro_precision'] for m in all_metrics.values()),\n",
    "    'micro_rec': max(m['micro_recall'] for m in all_metrics.values()),\n",
    "    'micro_f1': max(m['micro_f1'] for m in all_metrics.values()),\n",
    "}\n",
    "\n",
    "table_data = []\n",
    "for model in sorted_models:\n",
    "    metrics = all_metrics[model]\n",
    "    model_display = model.upper()\n",
    "    \n",
    "    def fmt(value, best):\n",
    "        val_str = f\"{int(value*100)}\\\\%\"\n",
    "        if abs(value - best) < 1e-9:  \n",
    "            return f\"\\\\textbf{{{val_str}}}\"\n",
    "        return val_str\n",
    "    \n",
    "    table_data.append([f\"{model_display} (n={metrics['n']})\", \"Positive\", \n",
    "                       fmt(metrics['precision'][0], best_values['pos_prec']),\n",
    "                       fmt(metrics['recall'][0], best_values['pos_rec']),\n",
    "                       fmt(metrics['f1'][0], best_values['pos_f1'])])\n",
    "    table_data.append([f\"{model_display} (n={metrics['n']})\", \"Negative\", \n",
    "                       fmt(metrics['precision'][1], best_values['neg_prec']),\n",
    "                       fmt(metrics['recall'][1], best_values['neg_rec']),\n",
    "                       fmt(metrics['f1'][1], best_values['neg_f1'])])\n",
    "    table_data.append([f\"{model_display} (n={metrics['n']})\", \"Neutral\", \n",
    "                       fmt(metrics['precision'][2], best_values['neu_prec']),\n",
    "                       fmt(metrics['recall'][2], best_values['neu_rec']),\n",
    "                       fmt(metrics['f1'][2], best_values['neu_f1'])])\n",
    "    table_data.append([f\"{model_display} (n={metrics['n']})\", \"Macro Avg.\", \n",
    "                       fmt(metrics['macro_precision'], best_values['macro_prec']),\n",
    "                       fmt(metrics['macro_recall'], best_values['macro_rec']),\n",
    "                       fmt(metrics['macro_f1'], best_values['macro_f1'])])\n",
    "    table_data.append([f\"{model_display} (n={metrics['n']})\", \"Micro Avg.\", \n",
    "                       fmt(metrics['micro_precision'], best_values['micro_prec']),\n",
    "                       fmt(metrics['micro_recall'], best_values['micro_rec']),\n",
    "                       fmt(metrics['micro_f1'], best_values['micro_f1'])])\n",
    "\n",
    "df = pd.DataFrame(table_data, columns=['Model', 'Class', 'Precision', 'Recall', 'F1-score'])\n",
    "\n",
    "latex_table = df.to_latex(\n",
    "    index=False, \n",
    "    escape=False,\n",
    "    column_format='|l|l|r|r|r|',\n",
    "    caption=\"Performance of transformer-based models on the gold dataset\",\n",
    "    label=\"tab:transformers_gold\"\n",
    ")\n",
    "\n",
    "lines = latex_table.split('\\n')\n",
    "processed_lines = []\n",
    "current_model = None\n",
    "row_count = 0\n",
    "first_model = True\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if 'Model &' in line and 'Class &' in line:\n",
    "        line = line.replace('Model', '\\\\textbf{Model}')\n",
    "        line = line.replace('Class', '\\\\textbf{Class}')\n",
    "        line = line.replace('Precision', '\\\\textbf{Precision}')\n",
    "        line = line.replace('Recall', '\\\\textbf{Recall}')\n",
    "        line = line.replace('F1-score', '\\\\textbf{F1-score}')\n",
    "        processed_lines.append(line)\n",
    "        continue\n",
    "    \n",
    "    if '&' in line and 'toprule' not in line and 'midrule' not in line and 'bottomrule' not in line:\n",
    "        parts = line.split('&')\n",
    "        if len(parts) >= 5:\n",
    "            model_name = parts[0].strip()\n",
    "            \n",
    "            if model_name and model_name != '':\n",
    "                if model_name != current_model:\n",
    "                    if not first_model:\n",
    "                        processed_lines.append('\\\\hline')\n",
    "                    first_model = False\n",
    "                    \n",
    "                    current_model = model_name\n",
    "                    row_count = 1\n",
    "                    parts[0] = f\" \\\\multirow{{5}}{{*}}{{{model_name}}}\"\n",
    "                else:\n",
    "                    row_count += 1\n",
    "                    parts[0] = \" \"\n",
    "                    \n",
    "                line = ' & '.join(parts)\n",
    "                \n",
    "                if row_count < 5 and line.strip().endswith('\\\\\\\\'):\n",
    "                    line = line.rstrip() + ' \\\\cline{2-5}'\n",
    "    \n",
    "    processed_lines.append(line)\n",
    "\n",
    "latex_table = '\\n'.join(processed_lines)\n",
    "\n",
    "latex_table = latex_table.replace('\\\\toprule', '\\\\hline')\n",
    "latex_table = latex_table.replace('\\\\midrule', '\\\\hline')\n",
    "latex_table = latex_table.replace('\\\\bottomrule', '\\\\hline')\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162b88c",
   "metadata": {},
   "source": [
    "## Premo Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051261d0",
   "metadata": {},
   "source": [
    "Load results from transformer models on the premo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000eda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "premo_results = {}\n",
    "for model in MODELS:\n",
    "    filepath = Path(f\"output_transformers/{model}_premo.json\")\n",
    "    if filepath.exists():\n",
    "        with open(filepath) as f:\n",
    "            premo_results[model] = json.load(f)\n",
    "    else:\n",
    "        print(f\"Missing file: {filepath}\")\n",
    "\n",
    "print(\"Loaded premo dataset results for models:\", list(premo_results.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0746e41",
   "metadata": {},
   "source": [
    "Generate LaTeX Table for Premo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aac8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_models_premo = sorted(premo_results.keys())\n",
    "\n",
    "all_metrics_premo = {}\n",
    "for model in sorted_models_premo:\n",
    "    data = premo_results[model]\n",
    "    metrics = data['metrics']\n",
    "    \n",
    "    all_metrics_premo[model] = {\n",
    "        'precision': [\n",
    "            metrics['per_class']['precision']['positive'],\n",
    "            metrics['per_class']['precision']['negative'],\n",
    "            metrics['per_class']['precision']['neutral']\n",
    "        ],\n",
    "        'recall': [\n",
    "            metrics['per_class']['recall']['positive'],\n",
    "            metrics['per_class']['recall']['negative'],\n",
    "            metrics['per_class']['recall']['neutral']\n",
    "        ],\n",
    "        'f1': [\n",
    "            metrics['per_class']['f1_score']['positive'],\n",
    "            metrics['per_class']['f1_score']['negative'],\n",
    "            metrics['per_class']['f1_score']['neutral']\n",
    "        ],\n",
    "        'macro_precision': metrics['macro']['precision'],\n",
    "        'micro_precision': metrics['micro']['precision'],\n",
    "        'macro_recall': metrics['macro']['recall'],\n",
    "        'micro_recall': metrics['micro']['recall'],\n",
    "        'macro_f1': metrics['macro']['f1_score'],\n",
    "        'micro_f1': metrics['micro']['f1_score'],\n",
    "        'n': data.get('n', 'N/A')\n",
    "    }\n",
    "\n",
    "best_values_premo = {\n",
    "    'pos_prec': max(m['precision'][0] for m in all_metrics_premo.values()),\n",
    "    'pos_rec': max(m['recall'][0] for m in all_metrics_premo.values()),\n",
    "    'pos_f1': max(m['f1'][0] for m in all_metrics_premo.values()),\n",
    "    'neg_prec': max(m['precision'][1] for m in all_metrics_premo.values()),\n",
    "    'neg_rec': max(m['recall'][1] for m in all_metrics_premo.values()),\n",
    "    'neg_f1': max(m['f1'][1] for m in all_metrics_premo.values()),\n",
    "    'neu_prec': max(m['precision'][2] for m in all_metrics_premo.values()),\n",
    "    'neu_rec': max(m['recall'][2] for m in all_metrics_premo.values()),\n",
    "    'neu_f1': max(m['f1'][2] for m in all_metrics_premo.values()),\n",
    "    'macro_prec': max(m['macro_precision'] for m in all_metrics_premo.values()),\n",
    "    'macro_rec': max(m['macro_recall'] for m in all_metrics_premo.values()),\n",
    "    'macro_f1': max(m['macro_f1'] for m in all_metrics_premo.values()),\n",
    "    'micro_prec': max(m['micro_precision'] for m in all_metrics_premo.values()),\n",
    "    'micro_rec': max(m['micro_recall'] for m in all_metrics_premo.values()),\n",
    "    'micro_f1': max(m['micro_f1'] for m in all_metrics_premo.values()),\n",
    "}\n",
    "\n",
    "table_data_premo = []\n",
    "for model in sorted_models_premo:\n",
    "    metrics = all_metrics_premo[model]\n",
    "    model_display = model.upper()\n",
    "    \n",
    "    def fmt(value, best):\n",
    "        val_str = f\"{int(value*100)}\\\\%\"\n",
    "        if abs(value - best) < 1e-9:  \n",
    "            return f\"\\\\textbf{{{val_str}}}\"\n",
    "        return val_str\n",
    "    \n",
    "    table_data_premo.append([f\"{model_display} (n={metrics['n']})\", \"Positive\", \n",
    "                       fmt(metrics['precision'][0], best_values_premo['pos_prec']),\n",
    "                       fmt(metrics['recall'][0], best_values_premo['pos_rec']),\n",
    "                       fmt(metrics['f1'][0], best_values_premo['pos_f1'])])\n",
    "    table_data_premo.append([f\"{model_display} (n={metrics['n']})\", \"Negative\", \n",
    "                       fmt(metrics['precision'][1], best_values_premo['neg_prec']),\n",
    "                       fmt(metrics['recall'][1], best_values_premo['neg_rec']),\n",
    "                       fmt(metrics['f1'][1], best_values_premo['neg_f1'])])\n",
    "    table_data_premo.append([f\"{model_display} (n={metrics['n']})\", \"Neutral\", \n",
    "                       fmt(metrics['precision'][2], best_values_premo['neu_prec']),\n",
    "                       fmt(metrics['recall'][2], best_values_premo['neu_rec']),\n",
    "                       fmt(metrics['f1'][2], best_values_premo['neu_f1'])])\n",
    "    table_data_premo.append([f\"{model_display} (n={metrics['n']})\", \"Macro Avg.\", \n",
    "                       fmt(metrics['macro_precision'], best_values_premo['macro_prec']),\n",
    "                       fmt(metrics['macro_recall'], best_values_premo['macro_rec']),\n",
    "                       fmt(metrics['macro_f1'], best_values_premo['macro_f1'])])\n",
    "    table_data_premo.append([f\"{model_display} (n={metrics['n']})\", \"Micro Avg.\", \n",
    "                       fmt(metrics['micro_precision'], best_values_premo['micro_prec']),\n",
    "                       fmt(metrics['micro_recall'], best_values_premo['micro_rec']),\n",
    "                       fmt(metrics['micro_f1'], best_values_premo['micro_f1'])])\n",
    "\n",
    "df_premo = pd.DataFrame(table_data_premo, columns=['Model', 'Class', 'Precision', 'Recall', 'F1-score'])\n",
    "\n",
    "latex_table_premo = df_premo.to_latex(\n",
    "    index=False, \n",
    "    escape=False,\n",
    "    column_format='|l|l|r|r|r|',\n",
    "    caption=\"Performance of transformer-based models on the premo dataset\",\n",
    "    label=\"tab:transformers_premo\"\n",
    ")\n",
    "\n",
    "lines = latex_table_premo.split('\\n')\n",
    "processed_lines = []\n",
    "current_model = None\n",
    "row_count = 0\n",
    "first_model = True\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if 'Model &' in line and 'Class &' in line:\n",
    "        line = line.replace('Model', '\\\\textbf{Model}')\n",
    "        line = line.replace('Class', '\\\\textbf{Class}')\n",
    "        line = line.replace('Precision', '\\\\textbf{Precision}')\n",
    "        line = line.replace('Recall', '\\\\textbf{Recall}')\n",
    "        line = line.replace('F1-score', '\\\\textbf{F1-score}')\n",
    "        processed_lines.append(line)\n",
    "        continue\n",
    "    \n",
    "    if '&' in line and 'toprule' not in line and 'midrule' not in line and 'bottomrule' not in line:\n",
    "        parts = line.split('&')\n",
    "        if len(parts) >= 5:\n",
    "            model_name = parts[0].strip()\n",
    "            \n",
    "            if model_name and model_name != '':\n",
    "                if model_name != current_model:\n",
    "                    if not first_model:\n",
    "                        processed_lines.append('\\\\hline')\n",
    "                    first_model = False\n",
    "                    \n",
    "                    current_model = model_name\n",
    "                    row_count = 1\n",
    "                    parts[0] = f\" \\\\multirow{{5}}{{*}}{{{model_name}}}\"\n",
    "                else:\n",
    "                    row_count += 1\n",
    "                    parts[0] = \" \"\n",
    "                    \n",
    "                line = ' & '.join(parts)\n",
    "                \n",
    "                if row_count < 5 and line.strip().endswith('\\\\\\\\'):\n",
    "                    line = line.rstrip() + ' \\\\cline{2-5}'\n",
    "    \n",
    "    processed_lines.append(line)\n",
    "\n",
    "latex_table_premo = '\\n'.join(processed_lines)\n",
    "\n",
    "latex_table_premo = latex_table_premo.replace('\\\\toprule', '\\\\hline')\n",
    "latex_table_premo = latex_table_premo.replace('\\\\midrule', '\\\\hline')\n",
    "latex_table_premo = latex_table_premo.replace('\\\\bottomrule', '\\\\hline')\n",
    "\n",
    "print(latex_table_premo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
