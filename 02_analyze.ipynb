{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loads the libraries necessary for the script.",
   "id": "9a4dc1172b51fa1b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loads the data from the dataset and included in it the data for all of the models (and prompt engineering techniques that were collected)",
   "id": "6753496f3579f4c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"data/dataset.json\") as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "MODELS = [\"gpt-4o-2024-05-13\", \"gpt-4o-mini-2024-07-18\", \"mistral-nemo:12b\", \"gemma2:9b\", \"llama3.1:8b\", \"mistral-small:22b\", \"gemma2:27b\", \"llama3.1:70b\"]\n",
    "\n",
    "for number in [0, 1, 3, \"cot\"]:\n",
    "    for model in MODELS:\n",
    "        run_name = model + f\"_{str(number)}shot\"\n",
    "        for index, message in enumerate(dataset):\n",
    "            if not Path(f\"output/{run_name}.json\").exists():\n",
    "                continue\n",
    "            with open(f\"output/{run_name}.json\") as file:\n",
    "                data = json.load(file)\n",
    "                try:\n",
    "                    if \"sentiment\" in data[index]:\n",
    "                        message[\"tools\"][run_name] = data[index]['sentiment']\n",
    "                    else:\n",
    "                        message[\"tools\"][run_name] = \"invalid\"\n",
    "                except:\n",
    "                    message[\"tools\"][run_name] = \"invalid\""
   ],
   "id": "4df3bf28d0ab59be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Created the data structures required by sklearn to generate a confusion matrix.",
   "id": "77bbb2d884265834"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "expected = [message[\"part2_aggregate\"][\"polarity\"] if message[\"part2_aggregate\"][\"polarity\"] != \"undefined\" else message[\"discussion_polarity\"] for message in dataset]\n",
    "\n",
    "actual = {}\n",
    "\n",
    "for tool in dataset[0][\"tools\"].keys():\n",
    "    actual[tool] = [x[\"tools\"][tool] for x in dataset]\n",
    "    \n",
    "    if tool == \"SentiCR\":\n",
    "        actual[tool] = [x if \"negative\" else \"neutral\" for x in actual[tool]]"
   ],
   "id": "a263b4330e51f080",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generated and prints the confusion matrix for each model and prompt engineering technique.",
   "id": "56b9979c13a58b74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "for tool in actual.keys():\n",
    "    cm = confusion_matrix(expected, actual[tool], labels=labels)\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    print(f\"Confusion Matrix for {tool}:\")\n",
    "    print(cm_df)\n",
    "    print()"
   ],
   "id": "dc48d58fc5ae37ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prints the precision, recall, and f1-score for each of the models and prompt engineering techniques, utilizing sklearn.",
   "id": "c61b1067968137a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for tool in actual.keys():\n",
    "    # Calculate Precision, Recall, and F1-score for each category\n",
    "    precision = precision_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    recall = recall_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    f1_scores = f1_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    \n",
    "    # Create a DataFrame for the metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1_scores\n",
    "    }, index=labels).round(2)\n",
    "    \n",
    "    print(f\"\\nMetrics per Category for {tool}:\")\n",
    "    print(metrics_df)\n",
    "    \n",
    "    macro_f1 = f1_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_f1 = f1_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    macro_precision = precision_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_precision = precision_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    macro_recall = recall_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_recall = recall_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    included_count = sum(1 for item in actual[tool] if item in labels)\n",
    "    discarded_count = len(actual[tool]) - included_count\n",
    "    \n",
    "    print(\"Macro Precision:\", round(macro_precision, 2))\n",
    "    print(\"Micro Precision:\", round(micro_precision,2))\n",
    "    print(\"Macro Recall:   \", round(macro_recall,2))\n",
    "    print(\"Micro Recall:   \", round(micro_recall,2))\n",
    "    print(\"Macro F1 Score: \", round(macro_f1,2))\n",
    "    print(\"Micro F1 Score: \", round(micro_f1,2))\n",
    "    print(\"N:\", included_count)"
   ],
   "id": "9be3dc9474b54bac",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
