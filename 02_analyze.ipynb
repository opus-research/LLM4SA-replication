{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-11T06:56:20.963603Z",
     "start_time": "2024-11-11T06:56:20.960637Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ],
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T06:56:34.171519Z",
     "start_time": "2024-11-11T06:56:20.972922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/dataset.json\") as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "MODELS = [\"gpt-4o-2024-05-13\", \"gpt-4o-mini-2024-07-18\", \"mistral-nemo:12b\", \"gemma2:9b\", \"llama3.1:8b\", \"mistral-small:22b\", \"gemma2:27b\", \"llama3.1:70b\"]\n",
    "\n",
    "for number in [0, 1, 3, \"cot\"]:\n",
    "    for model in MODELS:\n",
    "        run_name = model + f\"_{str(number)}shot\"\n",
    "        for index, message in enumerate(dataset):\n",
    "            if not Path(f\"output/{run_name}.json\").exists():\n",
    "                continue\n",
    "            with open(f\"output/{run_name}.json\") as file:\n",
    "                data = json.load(file)\n",
    "                try:\n",
    "                    if \"sentiment\" in data[index]:\n",
    "                        message[\"tools\"][run_name] = data[index]['sentiment']\n",
    "                    else:\n",
    "                        message[\"tools\"][run_name] = \"invalid\"\n",
    "                except:\n",
    "                    message[\"tools\"][run_name] = \"invalid\""
   ],
   "id": "4df3bf28d0ab59be",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T06:56:34.186308Z",
     "start_time": "2024-11-11T06:56:34.177796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "expected = [message[\"part2_aggregate\"][\"polarity\"] if message[\"part2_aggregate\"][\"polarity\"] != \"undefined\" else message[\"discussion_polarity\"] for message in dataset]\n",
    "\n",
    "actual = {}\n",
    "\n",
    "for tool in dataset[0][\"tools\"].keys():\n",
    "    actual[tool] = [x[\"tools\"][tool] for x in dataset]\n",
    "    \n",
    "    if tool == \"SentiCR\":\n",
    "        actual[tool] = [x if \"negative\" else \"neutral\" for x in actual[tool]]"
   ],
   "id": "a263b4330e51f080",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T06:56:34.271288Z",
     "start_time": "2024-11-11T06:56:34.192852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "for tool in actual.keys():\n",
    "    cm = confusion_matrix(expected, actual[tool], labels=labels)\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    print(f\"Confusion Matrix for {tool}:\")\n",
    "    print(cm_df)\n",
    "    print()"
   ],
   "id": "dc48d58fc5ae37ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for SentiStrength:\n",
      "          positive  negative  neutral\n",
      "positive       411        80       30\n",
      "negative        87       301       44\n",
      "neutral        245       287      306\n",
      "\n",
      "Confusion Matrix for SentiStrengthSE:\n",
      "          positive  negative  neutral\n",
      "positive       328       143       50\n",
      "negative        16       345       71\n",
      "neutral         70       304      464\n",
      "\n",
      "Confusion Matrix for SentiCR:\n",
      "          positive  negative  neutral\n",
      "positive         0        62      459\n",
      "negative         0       172      260\n",
      "neutral          0       168      670\n",
      "\n",
      "Confusion Matrix for DEVA:\n",
      "          positive  negative  neutral\n",
      "positive       377       103       41\n",
      "negative        49       304       79\n",
      "neutral        141       221      476\n",
      "\n",
      "Confusion Matrix for Senti4SD:\n",
      "          positive  negative  neutral\n",
      "positive       371        34      116\n",
      "negative        86       184      162\n",
      "neutral        200       103      535\n",
      "\n",
      "Confusion Matrix for gpt-4o-2024-05-13_0shot:\n",
      "          positive  negative  neutral\n",
      "positive       238         9      274\n",
      "negative         1       173      258\n",
      "neutral         17        40      781\n",
      "\n",
      "Confusion Matrix for gpt-4o-mini-2024-07-18_0shot:\n",
      "          positive  negative  neutral\n",
      "positive       343         5      173\n",
      "negative        15       122      295\n",
      "neutral         61        31      746\n",
      "\n",
      "Confusion Matrix for mistral-nemo:12b_0shot:\n",
      "          positive  negative  neutral\n",
      "positive       383        11      126\n",
      "negative        15       219      193\n",
      "neutral        101        98      637\n",
      "\n",
      "Confusion Matrix for gemma2:9b_0shot:\n",
      "          positive  negative  neutral\n",
      "positive       359        10      152\n",
      "negative        13       190      229\n",
      "neutral         94        80      663\n",
      "\n",
      "Confusion Matrix for llama3.1:8b_0shot:\n",
      "          positive  negative  neutral\n",
      "positive       445        18       58\n",
      "negative        77       242      113\n",
      "neutral        285       140      413\n",
      "\n",
      "Confusion Matrix for mistral-small:22b_0shot:\n",
      "          positive  negative  neutral\n",
      "positive       265         9      247\n",
      "negative         2       180      250\n",
      "neutral         21        62      754\n",
      "\n",
      "Confusion Matrix for gemma2:27b_0shot:\n",
      "          positive  negative  neutral\n",
      "positive       430        10       81\n",
      "negative        49       223      160\n",
      "neutral        195        94      548\n",
      "\n",
      "Confusion Matrix for llama3.1:70b_0shot:\n",
      "          positive  negative  neutral\n",
      "positive       405        10      106\n",
      "negative        27       174      231\n",
      "neutral        119        66      653\n",
      "\n",
      "Confusion Matrix for gpt-4o-2024-05-13_1shot:\n",
      "          positive  negative  neutral\n",
      "positive       422        22       77\n",
      "negative        32       270      130\n",
      "neutral         91       101      646\n",
      "\n",
      "Confusion Matrix for gpt-4o-mini-2024-07-18_1shot:\n",
      "          positive  negative  neutral\n",
      "positive       431        14       76\n",
      "negative        40       195      197\n",
      "neutral        157        63      618\n",
      "\n",
      "Confusion Matrix for mistral-nemo:12b_1shot:\n",
      "          positive  negative  neutral\n",
      "positive       411        14       96\n",
      "negative        20       267      142\n",
      "neutral         89        90      657\n",
      "\n",
      "Confusion Matrix for gemma2:9b_1shot:\n",
      "          positive  negative  neutral\n",
      "positive       436        16       69\n",
      "negative        43       241      148\n",
      "neutral        216       116      505\n",
      "\n",
      "Confusion Matrix for llama3.1:8b_1shot:\n",
      "          positive  negative  neutral\n",
      "positive       483        14       24\n",
      "negative       116       226       90\n",
      "neutral        420       114      304\n",
      "\n",
      "Confusion Matrix for mistral-small:22b_1shot:\n",
      "          positive  negative  neutral\n",
      "positive       325         9      187\n",
      "negative         8       228      196\n",
      "neutral         49        72      716\n",
      "\n",
      "Confusion Matrix for gemma2:27b_1shot:\n",
      "          positive  negative  neutral\n",
      "positive       408        12      101\n",
      "negative        34       257      141\n",
      "neutral        141        94      602\n",
      "\n",
      "Confusion Matrix for gpt-4o-2024-05-13_3shot:\n",
      "          positive  negative  neutral\n",
      "positive       384        13      124\n",
      "negative        14       211      207\n",
      "neutral         42        47      749\n",
      "\n",
      "Confusion Matrix for gpt-4o-mini-2024-07-18_3shot:\n",
      "          positive  negative  neutral\n",
      "positive       412        10       99\n",
      "negative        27       186      219\n",
      "neutral        133        55      650\n",
      "\n",
      "Confusion Matrix for mistral-nemo:12b_3shot:\n",
      "          positive  negative  neutral\n",
      "positive       398        10      113\n",
      "negative        18       207      204\n",
      "neutral         82        54      700\n",
      "\n",
      "Confusion Matrix for gemma2:9b_3shot:\n",
      "          positive  negative  neutral\n",
      "positive       429        15       77\n",
      "negative        36       229      167\n",
      "neutral        193       101      543\n",
      "\n",
      "Confusion Matrix for llama3.1:8b_3shot:\n",
      "          positive  negative  neutral\n",
      "positive       473        15       33\n",
      "negative       103       228      101\n",
      "neutral        379       110      349\n",
      "\n",
      "Confusion Matrix for mistral-small:22b_3shot:\n",
      "          positive  negative  neutral\n",
      "positive       282        10      229\n",
      "negative         4       203      225\n",
      "neutral         21        57      759\n",
      "\n",
      "Confusion Matrix for gemma2:27b_3shot:\n",
      "          positive  negative  neutral\n",
      "positive       392        14      115\n",
      "negative        30       240      162\n",
      "neutral        129        88      620\n",
      "\n",
      "Confusion Matrix for gpt-4o-2024-05-13_cotshot:\n",
      "          positive  negative  neutral\n",
      "positive       420        10       91\n",
      "negative         8       286      138\n",
      "neutral         60        73      705\n",
      "\n",
      "Confusion Matrix for gpt-4o-mini-2024-07-18_cotshot:\n",
      "          positive  negative  neutral\n",
      "positive       436         7       78\n",
      "negative        20       208      204\n",
      "neutral        119        47      672\n",
      "\n",
      "Confusion Matrix for mistral-nemo:12b_cotshot:\n",
      "          positive  negative  neutral\n",
      "positive       415        22       84\n",
      "negative        16       286      126\n",
      "neutral        115       111      610\n",
      "\n",
      "Confusion Matrix for gemma2:9b_cotshot:\n",
      "          positive  negative  neutral\n",
      "positive       448        12       61\n",
      "negative        47       221      164\n",
      "neutral        220        98      519\n",
      "\n",
      "Confusion Matrix for llama3.1:8b_cotshot:\n",
      "          positive  negative  neutral\n",
      "positive       487        19       15\n",
      "negative       126       242       64\n",
      "neutral        409       129      300\n",
      "\n",
      "Confusion Matrix for mistral-small:22b_cotshot:\n",
      "          positive  negative  neutral\n",
      "positive       307        10      204\n",
      "negative         6       249      177\n",
      "neutral         26        72      739\n",
      "\n",
      "Confusion Matrix for gemma2:27b_cotshot:\n",
      "          positive  negative  neutral\n",
      "positive       407        14      100\n",
      "negative        30       279      123\n",
      "neutral        115       103      619\n",
      "\n",
      "Confusion Matrix for llama3.1:70b_cotshot:\n",
      "          positive  negative  neutral\n",
      "positive       451        11       59\n",
      "negative        44       227      161\n",
      "neutral        163        83      592\n",
      "\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T06:56:35.370046Z",
     "start_time": "2024-11-11T06:56:34.277014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for tool in actual.keys():\n",
    "    # Calculate Precision, Recall, and F1-score for each category\n",
    "    precision = precision_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    recall = recall_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    f1_scores = f1_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    \n",
    "    # Create a DataFrame for the metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1_scores\n",
    "    }, index=labels).round(2)\n",
    "    \n",
    "    print(f\"\\nMetrics per Category for {tool}:\")\n",
    "    print(metrics_df)\n",
    "    \n",
    "    macro_f1 = f1_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_f1 = f1_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    macro_precision = precision_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_precision = precision_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    macro_recall = recall_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_recall = recall_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    included_count = sum(1 for item in actual[tool] if item in labels)\n",
    "    discarded_count = len(actual[tool]) - included_count\n",
    "    \n",
    "    print(\"Macro Precision:\", round(macro_precision, 2))\n",
    "    print(\"Micro Precision:\", round(micro_precision,2))\n",
    "    print(\"Macro Recall:   \", round(macro_recall,2))\n",
    "    print(\"Micro Recall:   \", round(micro_recall,2))\n",
    "    print(\"Macro F1 Score: \", round(macro_f1,2))\n",
    "    print(\"Micro F1 Score: \", round(micro_f1,2))\n",
    "    print(\"N:\", included_count)"
   ],
   "id": "9be3dc9474b54bac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics per Category for SentiStrength:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.55    0.79      0.65\n",
      "negative       0.45    0.70      0.55\n",
      "neutral        0.81    0.37      0.50\n",
      "Macro Precision: 0.6\n",
      "Micro Precision: 0.57\n",
      "Macro Recall:    0.62\n",
      "Micro Recall:    0.57\n",
      "Macro F1 Score:  0.57\n",
      "Micro F1 Score:  0.57\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for SentiStrengthSE:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.79    0.63      0.70\n",
      "negative       0.44    0.80      0.56\n",
      "neutral        0.79    0.55      0.65\n",
      "Macro Precision: 0.67\n",
      "Micro Precision: 0.63\n",
      "Macro Recall:    0.66\n",
      "Micro Recall:    0.63\n",
      "Macro F1 Score:  0.64\n",
      "Micro F1 Score:  0.63\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for SentiCR:\n",
      "          Precision  Recall  F1 Score\n",
      "positive        NaN     0.0      0.00\n",
      "negative       0.43     0.4      0.41\n",
      "neutral        0.48     0.8      0.60\n",
      "Macro Precision: 0.46\n",
      "Micro Precision: 0.47\n",
      "Macro Recall:    0.4\n",
      "Micro Recall:    0.47\n",
      "Macro F1 Score:  0.34\n",
      "Micro F1 Score:  0.47\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for DEVA:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.66    0.72      0.69\n",
      "negative       0.48    0.70      0.57\n",
      "neutral        0.80    0.57      0.66\n",
      "Macro Precision: 0.65\n",
      "Micro Precision: 0.65\n",
      "Macro Recall:    0.67\n",
      "Micro Recall:    0.65\n",
      "Macro F1 Score:  0.64\n",
      "Micro F1 Score:  0.65\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for Senti4SD:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.56    0.71      0.63\n",
      "negative       0.57    0.43      0.49\n",
      "neutral        0.66    0.64      0.65\n",
      "Macro Precision: 0.6\n",
      "Micro Precision: 0.61\n",
      "Macro Recall:    0.59\n",
      "Micro Recall:    0.61\n",
      "Macro F1 Score:  0.59\n",
      "Micro F1 Score:  0.61\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for gpt-4o-2024-05-13_0shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.93    0.46      0.61\n",
      "negative       0.78    0.40      0.53\n",
      "neutral        0.59    0.93      0.73\n",
      "Macro Precision: 0.77\n",
      "Micro Precision: 0.67\n",
      "Macro Recall:    0.6\n",
      "Micro Recall:    0.67\n",
      "Macro F1 Score:  0.62\n",
      "Micro F1 Score:  0.67\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for gpt-4o-mini-2024-07-18_0shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.82    0.66      0.73\n",
      "negative       0.77    0.28      0.41\n",
      "neutral        0.61    0.89      0.73\n",
      "Macro Precision: 0.74\n",
      "Micro Precision: 0.68\n",
      "Macro Recall:    0.61\n",
      "Micro Recall:    0.68\n",
      "Macro F1 Score:  0.62\n",
      "Micro F1 Score:  0.68\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for mistral-nemo:12b_0shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.77    0.74      0.75\n",
      "negative       0.67    0.51      0.58\n",
      "neutral        0.67    0.76      0.71\n",
      "Macro Precision: 0.7\n",
      "Micro Precision: 0.69\n",
      "Macro Recall:    0.67\n",
      "Micro Recall:    0.69\n",
      "Macro F1 Score:  0.68\n",
      "Micro F1 Score:  0.69\n",
      "N: 1783\n",
      "\n",
      "Metrics per Category for gemma2:9b_0shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.77    0.69      0.73\n",
      "negative       0.68    0.44      0.53\n",
      "neutral        0.64    0.79      0.70\n",
      "Macro Precision: 0.69\n",
      "Micro Precision: 0.68\n",
      "Macro Recall:    0.64\n",
      "Micro Recall:    0.68\n",
      "Macro F1 Score:  0.66\n",
      "Micro F1 Score:  0.68\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for llama3.1:8b_0shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.55    0.85      0.67\n",
      "negative       0.60    0.56      0.58\n",
      "neutral        0.71    0.49      0.58\n",
      "Macro Precision: 0.62\n",
      "Micro Precision: 0.61\n",
      "Macro Recall:    0.64\n",
      "Micro Recall:    0.61\n",
      "Macro F1 Score:  0.61\n",
      "Micro F1 Score:  0.61\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for mistral-small:22b_0shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.92    0.51      0.66\n",
      "negative       0.72    0.42      0.53\n",
      "neutral        0.60    0.90      0.72\n",
      "Macro Precision: 0.75\n",
      "Micro Precision: 0.67\n",
      "Macro Recall:    0.61\n",
      "Micro Recall:    0.67\n",
      "Macro F1 Score:  0.63\n",
      "Micro F1 Score:  0.67\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for gemma2:27b_0shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.64    0.83      0.72\n",
      "negative       0.68    0.52      0.59\n",
      "neutral        0.69    0.65      0.67\n",
      "Macro Precision: 0.67\n",
      "Micro Precision: 0.67\n",
      "Macro Recall:    0.67\n",
      "Micro Recall:    0.67\n",
      "Macro F1 Score:  0.66\n",
      "Micro F1 Score:  0.67\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for llama3.1:70b_0shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.74    0.78      0.76\n",
      "negative       0.70    0.40      0.51\n",
      "neutral        0.66    0.78      0.71\n",
      "Macro Precision: 0.7\n",
      "Micro Precision: 0.69\n",
      "Macro Recall:    0.65\n",
      "Micro Recall:    0.69\n",
      "Macro F1 Score:  0.66\n",
      "Micro F1 Score:  0.69\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for gpt-4o-2024-05-13_1shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.77    0.81      0.79\n",
      "negative       0.69    0.62      0.65\n",
      "neutral        0.76    0.77      0.76\n",
      "Macro Precision: 0.74\n",
      "Micro Precision: 0.75\n",
      "Macro Recall:    0.74\n",
      "Micro Recall:    0.75\n",
      "Macro F1 Score:  0.74\n",
      "Micro F1 Score:  0.75\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for gpt-4o-mini-2024-07-18_1shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.69    0.83      0.75\n",
      "negative       0.72    0.45      0.55\n",
      "neutral        0.69    0.74      0.71\n",
      "Macro Precision: 0.7\n",
      "Micro Precision: 0.69\n",
      "Macro Recall:    0.67\n",
      "Micro Recall:    0.69\n",
      "Macro F1 Score:  0.67\n",
      "Micro F1 Score:  0.69\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for mistral-nemo:12b_1shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.79    0.79      0.79\n",
      "negative       0.72    0.62      0.67\n",
      "neutral        0.73    0.78      0.76\n",
      "Macro Precision: 0.75\n",
      "Micro Precision: 0.75\n",
      "Macro Recall:    0.73\n",
      "Micro Recall:    0.75\n",
      "Macro F1 Score:  0.74\n",
      "Micro F1 Score:  0.75\n",
      "N: 1786\n",
      "\n",
      "Metrics per Category for gemma2:9b_1shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.63    0.84      0.72\n",
      "negative       0.65    0.56      0.60\n",
      "neutral        0.70    0.60      0.65\n",
      "Macro Precision: 0.66\n",
      "Micro Precision: 0.66\n",
      "Macro Recall:    0.67\n",
      "Micro Recall:    0.66\n",
      "Macro F1 Score:  0.65\n",
      "Micro F1 Score:  0.66\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for llama3.1:8b_1shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.47    0.93      0.63\n",
      "negative       0.64    0.52      0.58\n",
      "neutral        0.73    0.36      0.48\n",
      "Macro Precision: 0.61\n",
      "Micro Precision: 0.57\n",
      "Macro Recall:    0.6\n",
      "Micro Recall:    0.57\n",
      "Macro F1 Score:  0.56\n",
      "Micro F1 Score:  0.57\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for mistral-small:22b_1shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.85    0.62      0.72\n",
      "negative       0.74    0.53      0.62\n",
      "neutral        0.65    0.85      0.74\n",
      "Macro Precision: 0.75\n",
      "Micro Precision: 0.71\n",
      "Macro Recall:    0.67\n",
      "Micro Recall:    0.71\n",
      "Macro F1 Score:  0.69\n",
      "Micro F1 Score:  0.71\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for gemma2:27b_1shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.70    0.78      0.74\n",
      "negative       0.71    0.59      0.65\n",
      "neutral        0.71    0.72      0.72\n",
      "Macro Precision: 0.71\n",
      "Micro Precision: 0.71\n",
      "Macro Recall:    0.7\n",
      "Micro Recall:    0.71\n",
      "Macro F1 Score:  0.7\n",
      "Micro F1 Score:  0.71\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for gpt-4o-2024-05-13_3shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.87    0.74      0.80\n",
      "negative       0.78    0.49      0.60\n",
      "neutral        0.69    0.89      0.78\n",
      "Macro Precision: 0.78\n",
      "Micro Precision: 0.75\n",
      "Macro Recall:    0.71\n",
      "Micro Recall:    0.75\n",
      "Macro F1 Score:  0.73\n",
      "Micro F1 Score:  0.75\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for gpt-4o-mini-2024-07-18_3shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.72    0.79      0.75\n",
      "negative       0.74    0.43      0.54\n",
      "neutral        0.67    0.78      0.72\n",
      "Macro Precision: 0.71\n",
      "Micro Precision: 0.7\n",
      "Macro Recall:    0.67\n",
      "Micro Recall:    0.7\n",
      "Macro F1 Score:  0.67\n",
      "Micro F1 Score:  0.7\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for mistral-nemo:12b_3shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.80    0.76      0.78\n",
      "negative       0.76    0.48      0.59\n",
      "neutral        0.69    0.84      0.75\n",
      "Macro Precision: 0.75\n",
      "Micro Precision: 0.73\n",
      "Macro Recall:    0.69\n",
      "Micro Recall:    0.73\n",
      "Macro F1 Score:  0.71\n",
      "Micro F1 Score:  0.73\n",
      "N: 1786\n",
      "\n",
      "Metrics per Category for gemma2:9b_3shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.65    0.82      0.73\n",
      "negative       0.66    0.53      0.59\n",
      "neutral        0.69    0.65      0.67\n",
      "Macro Precision: 0.67\n",
      "Micro Precision: 0.67\n",
      "Macro Recall:    0.67\n",
      "Micro Recall:    0.67\n",
      "Macro F1 Score:  0.66\n",
      "Micro F1 Score:  0.67\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for llama3.1:8b_3shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.50    0.91      0.64\n",
      "negative       0.65    0.53      0.58\n",
      "neutral        0.72    0.42      0.53\n",
      "Macro Precision: 0.62\n",
      "Micro Precision: 0.59\n",
      "Macro Recall:    0.62\n",
      "Micro Recall:    0.59\n",
      "Macro F1 Score:  0.58\n",
      "Micro F1 Score:  0.59\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for mistral-small:22b_3shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.92    0.54      0.68\n",
      "negative       0.75    0.47      0.58\n",
      "neutral        0.63    0.91      0.74\n",
      "Macro Precision: 0.77\n",
      "Micro Precision: 0.69\n",
      "Macro Recall:    0.64\n",
      "Micro Recall:    0.69\n",
      "Macro F1 Score:  0.67\n",
      "Micro F1 Score:  0.69\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for gemma2:27b_3shot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.71    0.75      0.73\n",
      "negative       0.70    0.56      0.62\n",
      "neutral        0.69    0.74      0.71\n",
      "Macro Precision: 0.7\n",
      "Micro Precision: 0.7\n",
      "Macro Recall:    0.68\n",
      "Micro Recall:    0.7\n",
      "Macro F1 Score:  0.69\n",
      "Micro F1 Score:  0.7\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for gpt-4o-2024-05-13_cotshot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.86    0.81      0.83\n",
      "negative       0.78    0.66      0.71\n",
      "neutral        0.75    0.84      0.80\n",
      "Macro Precision: 0.8\n",
      "Micro Precision: 0.79\n",
      "Macro Recall:    0.77\n",
      "Micro Recall:    0.79\n",
      "Macro F1 Score:  0.78\n",
      "Micro F1 Score:  0.79\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for gpt-4o-mini-2024-07-18_cotshot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.76    0.84      0.80\n",
      "negative       0.79    0.48      0.60\n",
      "neutral        0.70    0.80      0.75\n",
      "Macro Precision: 0.75\n",
      "Micro Precision: 0.73\n",
      "Macro Recall:    0.71\n",
      "Micro Recall:    0.73\n",
      "Macro F1 Score:  0.72\n",
      "Micro F1 Score:  0.73\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for mistral-nemo:12b_cotshot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.76    0.80      0.78\n",
      "negative       0.68    0.66      0.67\n",
      "neutral        0.74    0.73      0.74\n",
      "Macro Precision: 0.73\n",
      "Micro Precision: 0.73\n",
      "Macro Recall:    0.73\n",
      "Micro Recall:    0.73\n",
      "Macro F1 Score:  0.73\n",
      "Micro F1 Score:  0.73\n",
      "N: 1785\n",
      "\n",
      "Metrics per Category for gemma2:9b_cotshot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.63    0.86      0.72\n",
      "negative       0.67    0.51      0.58\n",
      "neutral        0.70    0.62      0.66\n",
      "Macro Precision: 0.66\n",
      "Micro Precision: 0.66\n",
      "Macro Recall:    0.66\n",
      "Micro Recall:    0.66\n",
      "Macro F1 Score:  0.65\n",
      "Micro F1 Score:  0.66\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for llama3.1:8b_cotshot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.48    0.93      0.63\n",
      "negative       0.62    0.56      0.59\n",
      "neutral        0.79    0.36      0.49\n",
      "Macro Precision: 0.63\n",
      "Micro Precision: 0.57\n",
      "Macro Recall:    0.62\n",
      "Micro Recall:    0.57\n",
      "Macro F1 Score:  0.57\n",
      "Micro F1 Score:  0.57\n",
      "N: 1791\n",
      "\n",
      "Metrics per Category for mistral-small:22b_cotshot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.91    0.59      0.71\n",
      "negative       0.75    0.58      0.65\n",
      "neutral        0.66    0.88      0.75\n",
      "Macro Precision: 0.77\n",
      "Micro Precision: 0.72\n",
      "Macro Recall:    0.68\n",
      "Micro Recall:    0.72\n",
      "Macro F1 Score:  0.71\n",
      "Micro F1 Score:  0.72\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for gemma2:27b_cotshot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.74    0.78      0.76\n",
      "negative       0.70    0.65      0.67\n",
      "neutral        0.74    0.74      0.74\n",
      "Macro Precision: 0.73\n",
      "Micro Precision: 0.73\n",
      "Macro Recall:    0.72\n",
      "Micro Recall:    0.73\n",
      "Macro F1 Score:  0.72\n",
      "Micro F1 Score:  0.73\n",
      "N: 1790\n",
      "\n",
      "Metrics per Category for llama3.1:70b_cotshot:\n",
      "          Precision  Recall  F1 Score\n",
      "positive       0.69    0.87      0.77\n",
      "negative       0.71    0.53      0.60\n",
      "neutral        0.73    0.71      0.72\n",
      "Macro Precision: 0.71\n",
      "Micro Precision: 0.71\n",
      "Macro Recall:    0.7\n",
      "Micro Recall:    0.71\n",
      "Macro F1 Score:  0.7\n",
      "Micro F1 Score:  0.71\n",
      "N: 1791\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T06:56:35.376717Z",
     "start_time": "2024-11-11T06:56:35.375594Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f54a5e6403ed589b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
