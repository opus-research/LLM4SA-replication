{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4dc1172b51fa1b",
   "metadata": {},
   "source": [
    "Loads the libraries necessary for the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6753496f3579f4c9",
   "metadata": {},
   "source": [
    "Loads the data from the dataset and included in it the data for all of the models (and prompt engineering techniques that were collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3bf28d0ab59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset.json\") as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "MODELS = [\"gpt-4o-2024-05-13\", \"gpt-4o-mini-2024-07-18\", \"mistral-nemo-12b\", \"gemma2-9b\", \"llama3.1-8b\", \"mistral-small-22b\", \"gemma2-27b\", \"llama3.1-70b\", \"deepseek-r1-8b\", \"deepseek-r1-32b\"]\n",
    "\n",
    "for number in [0, 1, 3, \"cot\"]:\n",
    "    for model in MODELS:\n",
    "        run_name = model + f\"_{str(number)}shot\"\n",
    "        if not Path(f\"output_premo/{run_name}.json\").exists():\n",
    "            continue\n",
    "        with open(f\"output_premo/{run_name}.json\") as file:\n",
    "            data = json.load(file)\n",
    "        for index, message in enumerate(dataset):\n",
    "            try:\n",
    "                if \"sentiment\" in data[index]:\n",
    "                    message[\"tools\"][run_name] = data[index]['sentiment']\n",
    "                else:\n",
    "                    message[\"tools\"][run_name] = \"invalid\"\n",
    "            except:\n",
    "                message[\"tools\"][run_name] = \"invalid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbb2d884265834",
   "metadata": {},
   "source": [
    "Created the data structures required by sklearn to generate a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263b4330e51f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [message[\"part2_aggregate\"][\"polarity\"] if message[\"part2_aggregate\"][\"polarity\"] != \"undefined\" else message[\"discussion_polarity\"] for message in dataset]\n",
    "\n",
    "actual = {}\n",
    "\n",
    "for tool in dataset[0][\"tools\"].keys():\n",
    "    actual[tool] = [x[\"tools\"][tool] for x in dataset]\n",
    "    \n",
    "    if tool == \"SentiCR\":\n",
    "        actual[tool] = [x if \"negative\" else \"neutral\" for x in actual[tool]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9979c13a58b74",
   "metadata": {},
   "source": [
    "Generated and prints the confusion matrix for each model and prompt engineering technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48d58fc5ae37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "for tool in actual.keys():\n",
    "    cm = confusion_matrix(expected, actual[tool], labels=labels)\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    print(f\"Confusion Matrix for {tool}:\")\n",
    "    print(cm_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b1067968137a5",
   "metadata": {},
   "source": [
    "Prints the precision, recall, and f1-score for each of the models and prompt engineering techniques, utilizing sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3dc9474b54bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in actual.keys():\n",
    "    # Calculate Precision, Recall, and F1-score for each category\n",
    "    precision = precision_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    recall = recall_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    f1_scores = f1_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    \n",
    "    # Create a DataFrame for the metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1_scores\n",
    "    }, index=labels).round(2)\n",
    "    \n",
    "    print(f\"\\nMetrics per Category for {tool}:\")\n",
    "    print(metrics_df)\n",
    "    \n",
    "    macro_f1 = f1_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_f1 = f1_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    macro_precision = precision_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_precision = precision_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    macro_recall = recall_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_recall = recall_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    included_count = sum(1 for item in actual[tool] if item in labels)\n",
    "    discarded_count = len(actual[tool]) - included_count\n",
    "    \n",
    "    print(\"Macro Precision:\", round(macro_precision, 2))\n",
    "    print(\"Micro Precision:\", round(micro_precision,2))\n",
    "    print(\"Macro Recall:   \", round(macro_recall,2))\n",
    "    print(\"Micro Recall:   \", round(micro_recall,2))\n",
    "    print(\"Macro F1 Score: \", round(macro_f1,2))\n",
    "    print(\"Micro F1 Score: \", round(micro_f1,2))\n",
    "    print(\"N:\", included_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a5c691",
   "metadata": {},
   "source": [
    "Generate LaTeX Table (for RQ1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b952d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tools = {k: v for k, v in actual.items() if '0shot' in k}\n",
    "\n",
    "model_data = {}\n",
    "for tool in baseline_tools.keys():\n",
    "    model_name = tool.replace('_0shot', '')\n",
    "    model_data[model_name] = tool\n",
    "\n",
    "sorted_models = sorted(model_data.keys())\n",
    "\n",
    "all_metrics = {}\n",
    "for model in sorted_models:\n",
    "    tool = model_data[model]\n",
    "    precision = precision_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    recall = recall_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    f1_scores = f1_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "    \n",
    "    macro_f1 = f1_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_f1 = f1_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    macro_precision = precision_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_precision = precision_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    macro_recall = recall_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "    micro_recall = recall_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "    \n",
    "    included_count = sum(1 for item in actual[tool] if item in labels)\n",
    "    \n",
    "    all_metrics[model] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1_scores,\n",
    "        'macro_precision': macro_precision,\n",
    "        'micro_precision': micro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'micro_recall': micro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1,\n",
    "        'n': included_count\n",
    "    }\n",
    "\n",
    "best_values = {\n",
    "    'pos_prec': max(m['precision'][0] for m in all_metrics.values()),\n",
    "    'pos_rec': max(m['recall'][0] for m in all_metrics.values()),\n",
    "    'pos_f1': max(m['f1'][0] for m in all_metrics.values()),\n",
    "    'neg_prec': max(m['precision'][1] for m in all_metrics.values()),\n",
    "    'neg_rec': max(m['recall'][1] for m in all_metrics.values()),\n",
    "    'neg_f1': max(m['f1'][1] for m in all_metrics.values()),\n",
    "    'neu_prec': max(m['precision'][2] for m in all_metrics.values()),\n",
    "    'neu_rec': max(m['recall'][2] for m in all_metrics.values()),\n",
    "    'neu_f1': max(m['f1'][2] for m in all_metrics.values()),\n",
    "    'macro_prec': max(m['macro_precision'] for m in all_metrics.values()),\n",
    "    'macro_rec': max(m['macro_recall'] for m in all_metrics.values()),\n",
    "    'macro_f1': max(m['macro_f1'] for m in all_metrics.values()),\n",
    "    'micro_prec': max(m['micro_precision'] for m in all_metrics.values()),\n",
    "    'micro_rec': max(m['micro_recall'] for m in all_metrics.values()),\n",
    "    'micro_f1': max(m['micro_f1'] for m in all_metrics.values()),\n",
    "}\n",
    "\n",
    "table_data = []\n",
    "for model in sorted_models:\n",
    "    metrics = all_metrics[model]\n",
    "    model_display = model.replace('-2024-05-13', '').replace('-2024-07-18', '')\n",
    "    \n",
    "    def fmt(value, best):\n",
    "        val_str = f\"{int(value*100)}\\\\%\"\n",
    "        if value == best:\n",
    "            return f\"\\\\textbf{{{val_str}}}\"\n",
    "        return val_str\n",
    "    \n",
    "    table_data.append([f\"{model_display} (n={metrics['n']})\", \"Positive\", \n",
    "                       fmt(metrics['precision'][0], best_values['pos_prec']),\n",
    "                       fmt(metrics['recall'][0], best_values['pos_rec']),\n",
    "                       fmt(metrics['f1'][0], best_values['pos_f1'])])\n",
    "    table_data.append([f\"{model_display} (n={metrics['n']})\", \"Negative\", \n",
    "                       fmt(metrics['precision'][1], best_values['neg_prec']),\n",
    "                       fmt(metrics['recall'][1], best_values['neg_rec']),\n",
    "                       fmt(metrics['f1'][1], best_values['neg_f1'])])\n",
    "    table_data.append([f\"{model_display} (n={metrics['n']})\", \"Neutral\", \n",
    "                       fmt(metrics['precision'][2], best_values['neu_prec']),\n",
    "                       fmt(metrics['recall'][2], best_values['neu_rec']),\n",
    "                       fmt(metrics['f1'][2], best_values['neu_f1'])])\n",
    "    table_data.append([f\"{model_display} (n={metrics['n']})\", \"Macro Avg.\", \n",
    "                       fmt(metrics['macro_precision'], best_values['macro_prec']),\n",
    "                       fmt(metrics['macro_recall'], best_values['macro_rec']),\n",
    "                       fmt(metrics['macro_f1'], best_values['macro_f1'])])\n",
    "    table_data.append([f\"{model_display} (n={metrics['n']})\", \"Micro Avg.\", \n",
    "                       fmt(metrics['micro_precision'], best_values['micro_prec']),\n",
    "                       fmt(metrics['micro_recall'], best_values['micro_rec']),\n",
    "                       fmt(metrics['micro_f1'], best_values['micro_f1'])])\n",
    "\n",
    "df = pd.DataFrame(table_data, columns=['Model', 'Class', 'Precision', 'Recall', 'F1-score'])\n",
    "\n",
    "latex_table = df.to_latex(\n",
    "    index=False, \n",
    "    escape=False,\n",
    "    column_format='|l|l|r|r|r|',\n",
    "    caption=\"Zero-shot performance for all models tested, for the premo dataset\",\n",
    "    label=\"tab:results\"\n",
    ")\n",
    "\n",
    "lines = latex_table.split('\\n')\n",
    "processed_lines = []\n",
    "current_model = None\n",
    "row_count = 0\n",
    "first_model = True\n",
    "is_header = False\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if 'Model &' in line and 'Class &' in line:\n",
    "        is_header = True\n",
    "        line = line.replace('Model', '\\\\textbf{Model}')\n",
    "        line = line.replace('Class', '\\\\textbf{Class}')\n",
    "        line = line.replace('Precision', '\\\\textbf{Precision}')\n",
    "        line = line.replace('Recall', '\\\\textbf{Recall}')\n",
    "        line = line.replace('F1-score', '\\\\textbf{F1-score}')\n",
    "        processed_lines.append(line)\n",
    "        continue\n",
    "    \n",
    "    if '&' in line and 'toprule' not in line and 'midrule' not in line and 'bottomrule' not in line:\n",
    "        parts = line.split('&')\n",
    "        if len(parts) >= 5:\n",
    "            model_name = parts[0].strip()\n",
    "            \n",
    "            if model_name and model_name != '':\n",
    "                if model_name != current_model:\n",
    "                    if not first_model:\n",
    "                        processed_lines.append('\\\\hline')\n",
    "                    first_model = False\n",
    "                    \n",
    "                    current_model = model_name\n",
    "                    row_count = 1\n",
    "                    parts[0] = f\" \\\\multirow{{5}}{{*}}{{{model_name}}}\"\n",
    "                else:\n",
    "                    row_count += 1\n",
    "                    parts[0] = \" \"\n",
    "                    \n",
    "                line = ' & '.join(parts)\n",
    "                \n",
    "                if row_count < 5 and line.strip().endswith('\\\\\\\\'):\n",
    "                    line = line.rstrip() + ' \\\\cline{2-5}'\n",
    "    \n",
    "    processed_lines.append(line)\n",
    "\n",
    "latex_table = '\\n'.join(processed_lines)\n",
    "\n",
    "latex_table = latex_table.replace('\\\\toprule', '\\\\hline')\n",
    "latex_table = latex_table.replace('\\\\midrule', '\\\\hline')\n",
    "latex_table = latex_table.replace('\\\\bottomrule', '\\\\hline')\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25edb433",
   "metadata": {},
   "source": [
    "Generate LaTeX Tables for Prompt Engineering Techniques (for RQ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shot_type in ['1shot', '3shot', 'cotshot']:\n",
    "    shot_tools = {k: v for k, v in actual.items() if shot_type in k}\n",
    "    \n",
    "    shot_model_data = {}\n",
    "    for tool in shot_tools.keys():\n",
    "        model_name = tool.replace(f'_{shot_type}', '')\n",
    "        shot_model_data[model_name] = tool\n",
    "    \n",
    "    sorted_shot_models = sorted(shot_model_data.keys())\n",
    "    \n",
    "    shot_metrics = {}\n",
    "    for model in sorted_shot_models:\n",
    "        tool = shot_model_data[model]\n",
    "        precision = precision_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "        recall = recall_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "        f1_scores = f1_score(expected, actual[tool], labels=labels, average=None, zero_division=np.nan)\n",
    "        \n",
    "        macro_f1 = f1_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "        micro_f1 = f1_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "        macro_precision = precision_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "        micro_precision = precision_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "        macro_recall = recall_score(expected, actual[tool], average='macro', zero_division=np.nan, labels=labels)\n",
    "        micro_recall = recall_score(expected, actual[tool], average='micro', zero_division=np.nan, labels=labels)\n",
    "        \n",
    "        included_count = sum(1 for item in actual[tool] if item in labels)\n",
    "        \n",
    "        shot_metrics[model] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1_scores,\n",
    "            'macro_precision': macro_precision,\n",
    "            'micro_precision': micro_precision,\n",
    "            'macro_recall': macro_recall,\n",
    "            'micro_recall': micro_recall,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1,\n",
    "            'n': included_count\n",
    "        }\n",
    "    \n",
    "    best_shot_values = {\n",
    "        'pos_prec': max(m['precision'][0] for m in shot_metrics.values()),\n",
    "        'pos_rec': max(m['recall'][0] for m in shot_metrics.values()),\n",
    "        'pos_f1': max(m['f1'][0] for m in shot_metrics.values()),\n",
    "        'neg_prec': max(m['precision'][1] for m in shot_metrics.values()),\n",
    "        'neg_rec': max(m['recall'][1] for m in shot_metrics.values()),\n",
    "        'neg_f1': max(m['f1'][1] for m in shot_metrics.values()),\n",
    "        'neu_prec': max(m['precision'][2] for m in shot_metrics.values()),\n",
    "        'neu_rec': max(m['recall'][2] for m in shot_metrics.values()),\n",
    "        'neu_f1': max(m['f1'][2] for m in shot_metrics.values()),\n",
    "        'macro_prec': max(m['macro_precision'] for m in shot_metrics.values()),\n",
    "        'macro_rec': max(m['macro_recall'] for m in shot_metrics.values()),\n",
    "        'macro_f1': max(m['macro_f1'] for m in shot_metrics.values()),\n",
    "        'micro_prec': max(m['micro_precision'] for m in shot_metrics.values()),\n",
    "        'micro_rec': max(m['micro_recall'] for m in shot_metrics.values()),\n",
    "        'micro_f1': max(m['micro_f1'] for m in shot_metrics.values()),\n",
    "    }\n",
    "    \n",
    "    table_data = []\n",
    "    for model in sorted_shot_models:\n",
    "        if model not in all_metrics:\n",
    "            continue\n",
    "            \n",
    "        metrics = shot_metrics[model]\n",
    "        baseline = all_metrics[model]\n",
    "        model_display = model.replace('-2024-05-13', '').replace('-2024-07-18', '')\n",
    "        \n",
    "        def fmt_with_diff(value, baseline_value, best):\n",
    "            val_str = f\"{int(value*100)}\\\\%\"\n",
    "            if value == best:\n",
    "                val_str = f\"\\\\textbf{{{val_str}}}\"\n",
    "            diff = (value - baseline_value) * 100\n",
    "            rounded_diff = round(diff)\n",
    "            sign = \"+\" if rounded_diff > 0 else \"\"\n",
    "            val_str += f\" ({sign}{rounded_diff}\\\\%)\"\n",
    "            return val_str\n",
    "        \n",
    "        table_data.append([f\"{model_display} (n={metrics['n']})\", \"Positive\", \n",
    "                           fmt_with_diff(metrics['precision'][0], baseline['precision'][0], best_shot_values['pos_prec']),\n",
    "                           fmt_with_diff(metrics['recall'][0], baseline['recall'][0], best_shot_values['pos_rec']),\n",
    "                           fmt_with_diff(metrics['f1'][0], baseline['f1'][0], best_shot_values['pos_f1'])])\n",
    "        table_data.append([f\"{model_display} (n={metrics['n']})\", \"Negative\", \n",
    "                           fmt_with_diff(metrics['precision'][1], baseline['precision'][1], best_shot_values['neg_prec']),\n",
    "                           fmt_with_diff(metrics['recall'][1], baseline['recall'][1], best_shot_values['neg_rec']),\n",
    "                           fmt_with_diff(metrics['f1'][1], baseline['f1'][1], best_shot_values['neg_f1'])])\n",
    "        table_data.append([f\"{model_display} (n={metrics['n']})\", \"Neutral\", \n",
    "                           fmt_with_diff(metrics['precision'][2], baseline['precision'][2], best_shot_values['neu_prec']),\n",
    "                           fmt_with_diff(metrics['recall'][2], baseline['recall'][2], best_shot_values['neu_rec']),\n",
    "                           fmt_with_diff(metrics['f1'][2], baseline['f1'][2], best_shot_values['neu_f1'])])\n",
    "        table_data.append([f\"{model_display} (n={metrics['n']})\", \"Macro Avg.\", \n",
    "                           fmt_with_diff(metrics['macro_precision'], baseline['macro_precision'], best_shot_values['macro_prec']),\n",
    "                           fmt_with_diff(metrics['macro_recall'], baseline['macro_recall'], best_shot_values['macro_rec']),\n",
    "                           fmt_with_diff(metrics['macro_f1'], baseline['macro_f1'], best_shot_values['macro_f1'])])\n",
    "        table_data.append([f\"{model_display} (n={metrics['n']})\", \"Micro Avg.\", \n",
    "                           fmt_with_diff(metrics['micro_precision'], baseline['micro_precision'], best_shot_values['micro_prec']),\n",
    "                           fmt_with_diff(metrics['micro_recall'], baseline['micro_recall'], best_shot_values['micro_rec']),\n",
    "                           fmt_with_diff(metrics['micro_f1'], baseline['micro_f1'], best_shot_values['micro_f1'])])\n",
    "    \n",
    "    df = pd.DataFrame(table_data, columns=['Model', 'Class', 'Precision', 'Recall', 'F1-score'])\n",
    "    \n",
    "    shot_label = shot_type.replace('shot', '-shot') if 'cot' not in shot_type else 'CoT'\n",
    "    latex_table = df.to_latex(\n",
    "        index=False, \n",
    "        escape=False,\n",
    "        column_format='|l|l|r|r|r|',\n",
    "        caption=f\"Performance with {shot_label} prompting for the PRemo dataset (improvement vs baseline shown in parentheses)\",\n",
    "        label=f\"tab:results_premo_{shot_type}\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    lines = latex_table.split('\\n')\n",
    "    processed_lines = []\n",
    "    current_model = None\n",
    "    row_count = 0\n",
    "    first_model = True\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if 'Model &' in line and 'Class &' in line:\n",
    "            line = line.replace('Model', '\\\\textbf{Model}')\n",
    "            line = line.replace('Class', '\\\\textbf{Class}')\n",
    "            line = line.replace('Precision', '\\\\textbf{Precision}')\n",
    "            line = line.replace('Recall', '\\\\textbf{Recall}')\n",
    "            line = line.replace('F1-score', '\\\\textbf{F1-score}')\n",
    "            processed_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        if '&' in line and 'toprule' not in line and 'midrule' not in line and 'bottomrule' not in line:\n",
    "            parts = line.split('&')\n",
    "            if len(parts) >= 5:\n",
    "                model_name = parts[0].strip()\n",
    "                \n",
    "                if model_name and model_name != '':\n",
    "                    if model_name != current_model:\n",
    "                        if not first_model:\n",
    "                            processed_lines.append('\\\\hline')\n",
    "                        first_model = False\n",
    "                        \n",
    "                        current_model = model_name\n",
    "                        row_count = 1\n",
    "                        parts[0] = f\" \\\\multirow{{5}}{{*}}{{{model_name}}}\"\n",
    "                    else:\n",
    "                        row_count += 1\n",
    "                        parts[0] = \" \"\n",
    "                        \n",
    "                    line = ' & '.join(parts)\n",
    "                    \n",
    "                    if row_count < 5 and line.strip().endswith('\\\\\\\\'):\n",
    "                        line = line.rstrip() + ' \\\\cline{2-5}'\n",
    "        \n",
    "        processed_lines.append(line)\n",
    "    \n",
    "    latex_table = '\\n'.join(processed_lines)\n",
    "    \n",
    "    latex_table = latex_table.replace('\\\\toprule', '\\\\hline')\n",
    "    latex_table = latex_table.replace('\\\\midrule', '\\\\hline')\n",
    "    latex_table = latex_table.replace('\\\\bottomrule', '\\\\hline')\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Table for {shot_type}\")\n",
    "    print('='*80)\n",
    "    print(latex_table)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
