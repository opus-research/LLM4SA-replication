{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925d295d",
   "metadata": {},
   "source": [
    "# Randomness Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f129e",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd1774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T19:10:12.574999700Z",
     "start_time": "2026-01-30T19:10:12.218427900Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import openai\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import ollama\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "ollama_client = ollama.Client(timeout=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b2b8e",
   "metadata": {},
   "source": [
    "## Load Dataset and Sample Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e38b2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T19:10:17.200977800Z",
     "start_time": "2026-01-30T19:10:17.170059700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the gold dataset\n",
    "dataset = []\n",
    "with open(\"data/github_gold.csv\", newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=';')\n",
    "    for row in reader:\n",
    "        if 'Text' in row and row['Text'].strip() != '':\n",
    "            dataset.append(row['Text'])\n",
    "\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "\n",
    "# Sample 50 prompts for testing\n",
    "SAMPLE_SIZE = 50\n",
    "sample_indices = random.sample(range(len(dataset)), SAMPLE_SIZE)\n",
    "sample_prompts = [dataset[i] for i in sample_indices]\n",
    "\n",
    "print(f\"Selected {SAMPLE_SIZE} random samples at indices: {sorted(sample_indices)[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9ec81",
   "metadata": {},
   "source": [
    "## Helper Functions for Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a76f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T19:10:28.860989700Z",
     "start_time": "2026-01-30T19:10:28.841544800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_system_prompt():\n",
    "    return {\"role\": \"system\",\n",
    "            \"content\": dedent(f\"\"\"\n",
    "                You are a bot that classifies messages from Github pull requests. Classify the message as one of three types of sentiment: positive, neutral, and negative.\n",
    "\n",
    "                For classification purposes, we consider love and joy (and related emotions) positive, \\\n",
    "                anger, sadness, and fear, negative, surprise can be positive or negative depending on the context, and \\\n",
    "                neutral is considered the absense of any emotions.\n",
    "\n",
    "                Return the result one of the following JSONs: {{\"sentiment\": \"positive\"}}, {{\"sentiment\": \"negative\"}} OR {{\"sentiment\": \"neutral\"}}.\n",
    "            \"\"\")}\n",
    "\n",
    "def get_system_prompt_with_message(message):\n",
    "    return {\"role\": \"user\",\n",
    "            \"content\": dedent(f\"\"\"\n",
    "                You are a bot that classifies messages from Github pull requests. Classify the message as one of three types of sentiment: positive, neutral, and negative.\n",
    "\n",
    "                For classification purposes, we consider love and joy (and related emotions) positive, \\\n",
    "                anger, sadness, and fear, negative, surprise can be positive or negative depending on the context, and \\\n",
    "                neutral is considered the absense of any emotions.\n",
    "\n",
    "                Return the result one of the following JSONs: {{\"sentiment\": \"positive\"}}, {{\"sentiment\": \"negative\"}} OR {{\"sentiment\": \"neutral\"}}.\n",
    "\n",
    "                Classify the following message:\n",
    "                {message}\n",
    "            \"\"\")}\n",
    "\n",
    "def get_final_prompt(message):\n",
    "    return {\"role\": \"user\", \"content\": f\"Classify the following message:\\n{message}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87d315",
   "metadata": {},
   "source": [
    "## API Call Functions with Additional Stats (throughput)\n",
    "\n",
    "We opted to not use this additional data due to the how reasoning models (deepseek) inaccurately report token (since they do not report reasoning tokens). We also could not consistently measure GPT execution due to network overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891cc1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T19:10:35.098648Z",
     "start_time": "2026-01-30T19:10:35.073066500Z"
    }
   },
   "outputs": [],
   "source": [
    "def ask_chatgpt_with_stats(prompts, model, temperature=0.0):\n",
    "    start_time = time.time()\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=prompts,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=temperature\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    completion_tokens = response.usage.completion_tokens\n",
    "    prompt_tokens = response.usage.prompt_tokens\n",
    "    total_tokens = response.usage.total_tokens\n",
    "    \n",
    "    tokens_per_second = completion_tokens / elapsed_time if elapsed_time > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'content': response.choices[0].message.content,\n",
    "        'tokens_per_second': tokens_per_second,\n",
    "        'completion_tokens': completion_tokens,\n",
    "        'prompt_tokens': prompt_tokens,\n",
    "        'total_tokens': total_tokens,\n",
    "        'elapsed_time': elapsed_time\n",
    "    }\n",
    "\n",
    "def ask_ollama_with_stats(prompts, model, temperature=0.0):\n",
    "    retry_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            response = ollama_client.chat(\n",
    "                model=model, \n",
    "                messages=prompts, \n",
    "                format=\"json\", \n",
    "                stream=False,\n",
    "                options={\n",
    "                    \"temperature\": temperature,\n",
    "                    \"num_ctx\": 8192,\n",
    "                    \"num_predict\": -1\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            content = response['message']['content']\n",
    "            eval_count = response.get('eval_count', 0)\n",
    "            eval_duration = response.get('eval_duration', 0)\n",
    "            prompt_eval_count = response.get('prompt_eval_count', 0)\n",
    "            prompt_eval_duration = response.get('prompt_eval_duration', 0)\n",
    "            \n",
    "            generation_tokens_per_second = 0\n",
    "            if eval_duration > 0:\n",
    "                generation_tokens_per_second = eval_count / (eval_duration / 1e9)\n",
    "            \n",
    "            total_tokens = eval_count + prompt_eval_count\n",
    "            total_duration = eval_duration + prompt_eval_duration\n",
    "            total_tokens_per_second = 0\n",
    "            if total_duration > 0:\n",
    "                total_tokens_per_second = total_tokens / (total_duration / 1e9)\n",
    "            \n",
    "            return {\n",
    "                'content': content,\n",
    "                'tokens_per_second': generation_tokens_per_second,\n",
    "                'total_tokens_per_second': total_tokens_per_second, \n",
    "                'generation_tokens_per_second': generation_tokens_per_second, \n",
    "                'eval_count': eval_count,\n",
    "                'eval_duration': eval_duration,\n",
    "                'prompt_eval_count': prompt_eval_count,\n",
    "                'prompt_eval_duration': prompt_eval_duration,\n",
    "                'total_duration': total_duration\n",
    "            }\n",
    "        except Exception as e:\n",
    "            time.sleep(5)\n",
    "            print(f\"Failed with {e}. Retrying...\")\n",
    "            retry_count += 1\n",
    "            if retry_count > 5:\n",
    "                return None\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62991",
   "metadata": {},
   "source": [
    "## Part 1: Randomness Analysis - Run Models 3 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cdd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODELS = [\"gpt-4o-2024-05-13\", \"gpt-4o-mini-2024-07-18\"]\n",
    "OLLAMA_MODELS = [\"mistral-nemo:12b\", \"gemma2:9b\", \"llama3.1:8b\", \"mistral-small:22b\", \n",
    "                 \"gemma2:27b\", \"deepseek-r1:8b\", \"deepseek-r1:32b\", \"llama3.1:70b\"]\n",
    "\n",
    "randomness_results = {\n",
    "    'gpt': {},\n",
    "    'ollama': {}\n",
    "}\n",
    "\n",
    "throughput_data = []\n",
    "\n",
    "NUM_RUNS = 3\n",
    "\n",
    "print(f\"Testing {len(GPT_MODELS)} GPT models and {len(OLLAMA_MODELS)} Ollama models\")\n",
    "print(f\"Each model will be run {NUM_RUNS} times on {SAMPLE_SIZE} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67aea6a",
   "metadata": {},
   "source": [
    "### Run GPT Models (3 times each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631217ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in GPT_MODELS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing model: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model_results = []\n",
    "    model_throughput = []\n",
    "    \n",
    "    for run_num in range(NUM_RUNS):\n",
    "        run_results = []\n",
    "        run_throughput = []\n",
    "        print(f\"\\nRun {run_num + 1}/{NUM_RUNS}\")\n",
    "        \n",
    "        for message in tqdm(sample_prompts, desc=f\"{model} - Run {run_num + 1}\"):\n",
    "            prompt = [get_system_prompt()] + [get_final_prompt(message)]\n",
    "            \n",
    "            try:\n",
    "                response = ask_chatgpt_with_stats(prompt, model, temperature=0.0)\n",
    "                parsed = json.loads(response['content'])\n",
    "                sentiment = parsed.get('sentiment', 'unknown')\n",
    "                run_results.append(sentiment)\n",
    "                \n",
    "                run_throughput.append(response['tokens_per_second'])\n",
    "                throughput_data.append({\n",
    "                    'model': model,\n",
    "                    'run': run_num + 1,\n",
    "                    'tokens_per_second': response['tokens_per_second'],\n",
    "                    'completion_tokens': response['completion_tokens'],\n",
    "                    'prompt_tokens': response['prompt_tokens'],\n",
    "                    'total_tokens': response['total_tokens'],\n",
    "                    'elapsed_time': response['elapsed_time']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                run_results.append('error')\n",
    "                run_throughput.append(0)\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        model_results.append(run_results)\n",
    "        model_throughput.append(run_throughput)\n",
    "        \n",
    "        avg_throughput = np.mean([t for t in run_throughput if t > 0])\n",
    "        print(f\"Average throughput for run {run_num + 1}: {avg_throughput:.2f} tokens/sec\")\n",
    "    \n",
    "    randomness_results['gpt'][model] = model_results\n",
    "    print(f\"Completed {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e15a26",
   "metadata": {},
   "source": [
    "### Run Ollama Models (3 times each) with Throughput Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in OLLAMA_MODELS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing model: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model_results = []\n",
    "    model_throughput = []\n",
    "    \n",
    "    for run_num in range(NUM_RUNS):\n",
    "        run_results = []\n",
    "        run_throughput = []\n",
    "        print(f\"\\nRun {run_num + 1}/{NUM_RUNS}\")\n",
    "        \n",
    "        for idx, message in enumerate(tqdm(sample_prompts, desc=f\"{model} - Run {run_num + 1}\")):\n",
    "            prompt = [get_system_prompt_with_message(message)]\n",
    "            \n",
    "            is_warmup = (idx == 0)\n",
    "            \n",
    "            try:\n",
    "                response = ask_ollama_with_stats(prompt, model, temperature=0.0)\n",
    "                if response:\n",
    "                    parsed = json.loads(response['content'])\n",
    "                    sentiment = parsed.get('sentiment', 'unknown')\n",
    "                    run_results.append(sentiment)\n",
    "                    \n",
    "                    run_throughput.append(response['total_tokens_per_second'])\n",
    "                    throughput_data.append({\n",
    "                        'model': model,\n",
    "                        'run': run_num + 1,\n",
    "                        'message_idx': idx,\n",
    "                        'is_warmup': is_warmup,\n",
    "                        'tokens_per_second': response['tokens_per_second'], \n",
    "                        'total_tokens_per_second': response['total_tokens_per_second'],\n",
    "                        'generation_tokens_per_second': response['generation_tokens_per_second'],\n",
    "                        'eval_count': response['eval_count'],\n",
    "                        'eval_duration_ns': response['eval_duration'],\n",
    "                        'prompt_eval_count': response['prompt_eval_count'],\n",
    "                        'prompt_eval_duration_ns': response['prompt_eval_duration'],\n",
    "                        'total_duration_ns': response['total_duration']\n",
    "                    })\n",
    "                else:\n",
    "                    run_results.append('error')\n",
    "                    run_throughput.append(0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                run_results.append('error')\n",
    "                run_throughput.append(0)\n",
    "        \n",
    "        model_results.append(run_results)\n",
    "        model_throughput.append(run_throughput)\n",
    "        \n",
    "        avg_throughput = np.mean([t for t in run_throughput if t > 0])\n",
    "        print(f\"Average total throughput for run {run_num + 1}: {avg_throughput:.2f} tokens/sec\")\n",
    "    \n",
    "    randomness_results['ollama'][model] = model_results\n",
    "    print(f\"Completed {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcb5a1",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce135491",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"output_throughput\").mkdir(exist_ok=True)\n",
    "with open(\"output_throughput/randomness_results.json\", \"w\") as f:\n",
    "    json.dump(randomness_results, f, indent=2)\n",
    "\n",
    "throughput_df = pd.DataFrame(throughput_data)\n",
    "throughput_df.to_csv(\"output_throughput/throughput_data.csv\", index=False)\n",
    "\n",
    "print(\"Results saved to output_throughput/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a9727",
   "metadata": {},
   "source": [
    "## Part 2: Consistency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_metrics = []\n",
    "\n",
    "def calculate_agreement(run1, run2):\n",
    "    valid_pairs = [(r1, r2) for r1, r2 in zip(run1, run2) if r1 != 'error' and r2 != 'error']\n",
    "    if not valid_pairs:\n",
    "        return 0, 0\n",
    "    \n",
    "    r1_vals, r2_vals = zip(*valid_pairs)\n",
    "    accuracy = accuracy_score(r1_vals, r2_vals)\n",
    "    kappa = cohen_kappa_score(r1_vals, r2_vals)\n",
    "    return accuracy, kappa\n",
    "\n",
    "for model, runs in randomness_results['gpt'].items():\n",
    "    agreements = []\n",
    "    kappas = []\n",
    "    \n",
    "    for i in range(len(runs)):\n",
    "        for j in range(i+1, len(runs)):\n",
    "            acc, kappa = calculate_agreement(runs[i], runs[j])\n",
    "            agreements.append(acc)\n",
    "            kappas.append(kappa)\n",
    "    \n",
    "    consistency_metrics.append({\n",
    "        'model': model,\n",
    "        'model_type': 'GPT',\n",
    "        'avg_accuracy': np.mean(agreements),\n",
    "        'avg_kappa': np.mean(kappas),\n",
    "        'min_accuracy': np.min(agreements),\n",
    "        'max_accuracy': np.max(agreements),\n",
    "        'std_accuracy': np.std(agreements),\n",
    "        'min_kappa': np.min(kappas),\n",
    "        'max_kappa': np.max(kappas),\n",
    "        'std_kappa': np.std(kappas)\n",
    "    })\n",
    "\n",
    "for model, runs in randomness_results['ollama'].items():\n",
    "    agreements = []\n",
    "    kappas = []\n",
    "    \n",
    "    for i in range(len(runs)):\n",
    "        for j in range(i+1, len(runs)):\n",
    "            acc, kappa = calculate_agreement(runs[i], runs[j])\n",
    "            agreements.append(acc)\n",
    "            kappas.append(kappa)\n",
    "    \n",
    "    consistency_metrics.append({\n",
    "        'model': model,\n",
    "        'model_type': 'Ollama',\n",
    "        'avg_accuracy': np.mean(agreements),\n",
    "        'avg_kappa': np.mean(kappas),\n",
    "        'min_accuracy': np.min(agreements),\n",
    "        'max_accuracy': np.max(agreements),\n",
    "        'std_accuracy': np.std(agreements),\n",
    "        'min_kappa': np.min(kappas),\n",
    "        'max_kappa': np.max(kappas),\n",
    "        'std_kappa': np.std(kappas)\n",
    "    })\n",
    "\n",
    "consistency_df = pd.DataFrame(consistency_metrics)\n",
    "consistency_df = consistency_df.sort_values('avg_kappa', ascending=False)\n",
    "print(\"\\nConsistency Metrics (sorted by Cohen's Kappa):\")\n",
    "print(consistency_df.to_string(index=False))\n",
    "\n",
    "pd.save_csv(consistency_df, \"output_throughput/consistency_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45d09d",
   "metadata": {},
   "source": [
    "### Visualize Consistency Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f77a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "\n",
    "consistency_df_sorted = consistency_df.sort_values('avg_kappa')\n",
    "colors = ['#2ecc71' if mt == 'GPT' else '#3498db' for mt in consistency_df_sorted['model_type']]\n",
    "\n",
    "ax.barh(range(len(consistency_df_sorted)), consistency_df_sorted['avg_kappa'], color=colors)\n",
    "ax.set_yticks(range(len(consistency_df_sorted)))\n",
    "ax.set_yticklabels(consistency_df_sorted['model'])\n",
    "ax.set_xlabel(\"Cohen's Kappa (Average across run pairs)\")\n",
    "ax.set_title(\"Model Consistency: Cohen's Kappa Between Runs\")\n",
    "ax.axvline(x=0.8, color='red', linestyle='--', alpha=0.5, label='High Agreement (Îº>0.8)')\n",
    "ax.legend()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output_throughput/consistency_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Consistency visualization saved to output_throughput/consistency_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
