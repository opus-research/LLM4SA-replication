{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917c55cbcd5f3ca5",
   "metadata": {},
   "source": [
    "Loads the libraries necessary for the script and also loads the environment variables in the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "import ollama\n",
    "import sqlite3\n",
    "import hashlib\n",
    "\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "CACHE_DB = Path(\"temp/premo_cache.db\")\n",
    "\n",
    "def init_cache(db_path=CACHE_DB):\n",
    "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path, timeout=30, isolation_level=None)\n",
    "    conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    conn.execute(\"PRAGMA synchronous=FULL;\")\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS cache (\n",
    "            key TEXT PRIMARY KEY,\n",
    "            model TEXT,\n",
    "            prompt_hash TEXT,\n",
    "            result TEXT,\n",
    "            ts INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    return conn\n",
    "\n",
    "def hash_prompt(prompt):\n",
    "    return hashlib.sha256(json.dumps(prompt, sort_keys=True, ensure_ascii=False).encode('utf-8')).hexdigest()\n",
    "\n",
    "def make_key(message_id, model, shot_label):\n",
    "    return f\"{message_id}::{model}::{shot_label}\"\n",
    "\n",
    "def get_cached(conn, key):\n",
    "    cur = conn.execute(\"SELECT result FROM cache WHERE key = ?\", (key,))\n",
    "    row = cur.fetchone()\n",
    "    return json.loads(row[0]) if row else None\n",
    "\n",
    "def set_cache(conn, key, model, prompt, result):\n",
    "    ph = hash_prompt(prompt)\n",
    "    ts = int(time.time())\n",
    "    conn.execute(\n",
    "        \"INSERT OR REPLACE INTO cache (key, model, prompt_hash, result, ts) VALUES (?, ?, ?, ?, ?)\",\n",
    "        (key, model, ph, json.dumps(result, ensure_ascii=False), ts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d40aaa7f33a876",
   "metadata": {},
   "source": [
    "Setup for utilizing the OpenAI and Ollama APIs. Also loads the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cb4d415168c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_conn = init_cache()\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "ollama_client = ollama.Client(timeout=120)\n",
    "\n",
    "with open(\"data/dataset.json\") as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f980d1511fe436d",
   "metadata": {},
   "source": [
    "Defines helper methods for asking questions to GPT and Ollama. The ollama version contains a retry part, since, especially on windows, ollama tends to fail to response sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb380c0681f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatgpt(prompts, model):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=prompts,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "        \n",
    "def ask_ollama(prompts, model):\n",
    "    retry_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            response = ollama_client.chat(model=model, messages=prompts, format=\"json\", stream=False,\n",
    "            options={\n",
    "                \"temperature\": 0,\n",
    "                \"num_ctx\": 8192,\n",
    "                \"num_predict\": -1\n",
    "            })\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            time.sleep(5)\n",
    "            print(f\"Failed with {e}. Retrying...\")\n",
    "            retry_count += 1\n",
    "            if retry_count > 5:\n",
    "                return None\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a77bc4503a5017",
   "metadata": {},
   "source": [
    "This cell defines helper methods that generate the prompt depending on the structure utilized by the model (OpenAI and Ollama can differ) and the prompt engineering technique utilized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01ec8b38d66708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_prompt():\n",
    "    return {\"role\": \"system\",\n",
    "            \"content\": dedent(f\"\"\"\n",
    "                You are a bot that classifies messages from Github pull requests. Classify the message as one of three types of sentiment: positive, neutral, and negative.\n",
    "\n",
    "                For classification purposes, we consider love and joy (and related emotions) positive, \\\n",
    "                anger, sadness, and fear, negative, surprise can be positive or negative depending on the context, and \\\n",
    "                neutral is considered the absense of any emotions.\n",
    "\n",
    "                Return the result one of the following JSONs: {{\"sentiment\": \"positive\"}}, {{\"sentiment\": \"negative\"}} OR {{\"sentiment\": \"neutral\"}}.\n",
    "            \"\"\")}\n",
    "\n",
    "def get_system_prompt_with_message(message):\n",
    "    return {\"role\": \"user\",\n",
    "            \"content\": dedent(f\"\"\"\n",
    "                You are a bot that classifies messages from Github pull requests. Classify the message as one of three types of sentiment: positive, neutral, and negative.\n",
    "\n",
    "                For classification purposes, we consider love and joy (and related emotions) positive, \\\n",
    "                anger, sadness, and fear, negative, surprise can be positive or negative depending on the context, and \\\n",
    "                neutral is considered the absense of any emotions.\n",
    "\n",
    "                Return the result one of the following JSONs: {{\"sentiment\": \"positive\"}}, {{\"sentiment\": \"negative\"}} OR {{\"sentiment\": \"neutral\"}}.\n",
    "                \n",
    "                Message: \"{message}\"\n",
    "            \"\"\")}\n",
    "\n",
    "def get_system_prompt_with_message_and_examples(message, num_examples, cot=False):\n",
    "    return {\"role\": \"user\",\n",
    "            \"content\": dedent(f\"\"\"\n",
    "                You are a bot that classifies messages from Github pull requests. Classify the message as one of three types of sentiment: positive, neutral, and negative.\n",
    "\n",
    "                For classification purposes, we consider love and joy (and related emotions) positive, \\\n",
    "                anger, sadness, and fear, negative, surprise can be positive or negative depending on the context, and \\\n",
    "                neutral is considered the absense of any emotions.\n",
    "\n",
    "                Return the result one of the following JSONs: {{\"sentiment\": \"positive\"}}, {{\"sentiment\": \"negative\"}} OR {{\"sentiment\": \"neutral\"}}.\n",
    "                \n",
    "                Here are some examples to help you get started:\n",
    "                {get_example_strings(num_examples, cot)}\n",
    "                \n",
    "                Message: \"{message}\"\n",
    "            \"\"\")}\n",
    "\n",
    "def get_examples(num_positive, num_negative, num_neutral):\n",
    "    positive = [\n",
    "        {\n",
    "            \"message\": \"Thinking out loud I'm not sure whether a subclass would be a good way to go or not, but happy to discuss further. Thanks!\",\n",
    "            \"reasoning\": \"Analyzing the tone of the message, we notice that the author is satisfied with what was implemented, and the use of 'thinking out loud' suggests a collaborative approach by openly exploring ideas; the phrase 'I'm not sure' shows openness to suggestions, and 'happy to discuss further' reveals a positive disposition for dialogue and adjustment, indicating that the author values feedback; finally, 'Thanks!' conveys gratitude and reinforces the tone of appreciation and cooperation.\"\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"Thanks, Your contribution is now merged in with a polish commit.\",\n",
    "            \"reasoning\": \"Observing the structure of the message, we notice that 'Thanks' establishes a tone of gratitude, and the expression 'Your contribution' emphasizes the value of the recipient's participation, creating a sense of recognition; the word 'merged' suggests approval, since the work was integrated into the main project,. These elements together reinforce a tone of approval and appreciation for the recipient's effort.\"\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"I did polish it a bit more, thanks Eddu!\",\n",
    "            \"reasoning\": \"The 'thanks Eddu!' suggesting camaraderie and gratitude, probably for feedback that was given in the pull request. By recognizing the importance of Eddu's feedback in a friendly way and acting on it, the author conveys a tone of cooperation and friendship.\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    neutral = [\n",
    "        {\n",
    "            \"message\": \"Quick check using python awscli.\",\n",
    "            \"reasoning\": \"Analyzing the phrase 'Quick check,' we see that it describes a brief verification without any quality or emotional judgment, suggesting an objective communication; 'Using python awscli' describes the tool and language used, keeping the focus on functionality; the tone is descriptive and informative, without emotional inclination, and the language is purely technical and direct, intended to share a detail without expecting a reaction.\"\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"Should this be `AsyncComponent` maybe?\",\n",
    "            \"reasoning\": \"The author of the message is providing, in a purely technical manner, feedback for a change that he believes should be done in the pull request. \"\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"Can you please give a short update when do you think the next Guice release will be cut? Are there any plans?\",\n",
    "            \"reasoning\": \"Looking at the excerpt 'Can you please give a short update,' we see it is a polite and straightforward request, without emotional tone; the phrase 'when do you think the next Guice release will be cut?' seeks information without implying urgency or frustration, and 'Are there any plans?' reinforces the tone of curiosity and inquiry, keeping the focus on information. The choice of words indicates interest but without any emotional inclination.\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    negative = [\n",
    "        {\n",
    "            \"message\": \"Sorry, test case was mistaken.\",\n",
    "            \"reasoning\": \"Observing the use of 'Sorry,' we notice an expression of regret that suggests the author feels responsible for the error.\"\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"Oh, you know what? I'm stupid. I'm actually using `http-server`. Please, accept my apologies \",\n",
    "            \"reasoning\": \"In this message, the author utilizes a self deprecating expression (I'm stupid) and then apologizes, likely to the reviewer.\"\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"Hmm, ok that is unfortunate. I have split support into a separate pull request (#2071). If you want you can close this pull request here then, if you don't think there is any chance this will be integrated.\",\n",
    "            \"reasoning\": \"Analyzing 'unfortunate,' we notice an expression of disappointment with the situation; the phrase 'If you want you can close this pull request' suggests resignation, indicating that the author has low expectations about the integration of the original request; the phrase 'if you don't think there is any chance' emphasizes a pessimistic, almost resigned tone regarding the pull request's outcome.\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return positive[0:num_positive], neutral[0:num_neutral], negative[0:num_negative]\n",
    "\n",
    "def get_example_shots(num):\n",
    "    \n",
    "    classes = {}\n",
    "    example_shots = []\n",
    "    \n",
    "    classes['positive'], classes['neutral'], classes['negative'] = get_examples(num, num, num)\n",
    "    \n",
    "    for key, value in classes.items():\n",
    "        for message in value:\n",
    "            example_shots.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message[\"message\"],\n",
    "            })\n",
    "            example_shots.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f'{{\"sentiment\": \"{key}\"}}',\n",
    "            })\n",
    "        \n",
    "    return example_shots\n",
    "        \n",
    "def get_example_strings(num, cot=False):\n",
    "    message = \"\"\n",
    "    classes = {}\n",
    "    \n",
    "    classes['positive'], classes['neutral'], classes['negative'] = get_examples(num, num, num)\n",
    "    \n",
    "    i = 1\n",
    "    for key, value in classes.items():\n",
    "        for msg in value:\n",
    "            message += f'Example {i}: {msg[\"message\"]}\\nResponse for Example {i}: {{\"sentiment\": \"{key}\"}}\\n'\n",
    "            if cot:\n",
    "                message += f'Reasoning for Example {i}: {msg[\"reasoning\"]}\\n'\n",
    "            i += 1\n",
    "            \n",
    "    return message\n",
    "\n",
    "def get_final_prompt(message):\n",
    "    return {\"role\": \"user\",\n",
    "            \"content\": message,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63e5d73edfa15a",
   "metadata": {},
   "source": [
    "# Data collection for the first research question (RQ1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c0b25",
   "metadata": {},
   "source": [
    "This first cell collects data from the GPT models (zero-shot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d208d29d266eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "GPT_MODELS = [\"gpt-4o-2024-05-13\"]\n",
    "\n",
    "for model in GPT_MODELS:\n",
    "    run_name = model + f\"_0shot\" + \".json\"\n",
    "    if Path(\"output_premo/\"+ run_name).exists():\n",
    "        continue\n",
    "    for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "        key = make_key(index, model, \"0shot\")\n",
    "        if key in cached_keys:\n",
    "            results[index] = get_cached(cache_conn, key)\n",
    "            continue\n",
    "        prompt = [get_system_prompt()] + [get_final_prompt(message[\"raw_message\"])]\n",
    "        model_response = ask_chatgpt(prompt, model)\n",
    "        try:\n",
    "            parsed = json.loads(model_response)\n",
    "            if parsed:\n",
    "                results[index] = parsed\n",
    "                set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                cached_keys.add(key)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    with open(f\"output_premo/{run_name}\", \"w\") as jsonfile:\n",
    "        json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901e8e3904420d2",
   "metadata": {},
   "source": [
    "This second cell collects data from the GPT models (zero-shot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71789222527e1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "GPT_MODELS = [\"gpt-4o-mini-2024-07-18\"]\n",
    "\n",
    "for model in GPT_MODELS:\n",
    "    run_name = model + f\"_0shot\" + \".json\"\n",
    "    if Path(\"output_premo/\" + run_name).exists():\n",
    "        continue\n",
    "    for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "        key = make_key(index, model, \"0shot\")\n",
    "        if key in cached_keys:\n",
    "            results[index] = get_cached(cache_conn, key)\n",
    "            continue\n",
    "        prompt = [get_system_prompt()] + [get_final_prompt(message[\"raw_message\"])]\n",
    "        model_response = ask_chatgpt(prompt, model)\n",
    "        try:\n",
    "            parsed = json.loads(model_response)\n",
    "            if parsed:\n",
    "                results[index] = parsed\n",
    "                set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                cached_keys.add(key)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "with open(f\"output_premo/{run_name}\", \"w\") as jsonfile:\n",
    "    json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7090ed3773afa7",
   "metadata": {},
   "source": [
    "This third cell collects the data from the local models, without providing any examples (zero-shot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b1fbff5a4feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "OLLAMA_MODELS = [\"mistral-nemo:12b\", \"gemma2:9b\", \"llama3.1:8b\", \"mistral-small:22b\", \"gemma2:27b\", \"deepseek-r1:8b\", \"deepseek-r1:32b\", \"llama3.1:70b\"]\n",
    "\n",
    "for model in OLLAMA_MODELS:\n",
    "    run_name = model.replace(\":\", \"-\") + f\"_0shot\" + \".json\"\n",
    "    if Path(\"output_premo/\" + run_name).exists():\n",
    "        continue\n",
    "    for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "        key = make_key(index, model, \"0shot\")\n",
    "        if key in cached_keys:\n",
    "            results[index] = get_cached(cache_conn, key)\n",
    "            continue\n",
    "        prompt = [get_system_prompt_with_message(message[\"raw_message\"])]\n",
    "        model_response = ask_ollama(prompt, model)\n",
    "        try:\n",
    "            parsed = json.loads(model_response)\n",
    "            if parsed:\n",
    "                results[index] = parsed\n",
    "                set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                cached_keys.add(key)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    with open(f\"output_premo/{run_name}\", \"w\") as jsonfile:\n",
    "        json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c01e7f6fe2a15",
   "metadata": {},
   "source": [
    "# Data collection for the second research question (RQ2)\n",
    "\n",
    "This first cell collects the data for the OpenAI models, for the one-shot and few-shot cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91876f6e6e07a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "GPT_MODELS = [\"gpt-4o-mini-2024-07-18\", \"gpt-4o-2024-05-13\"]\n",
    "\n",
    "for model in GPT_MODELS:\n",
    "    for number in [1, 3]:\n",
    "        run_name = model + f\"_{str(number)}shot\" + \".json\"\n",
    "        if Path(\"output_premo/\"+ run_name).exists():\n",
    "            continue\n",
    "        for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "            key = make_key(index, model, f\"{str(number)}shot\")\n",
    "            if key in cached_keys:\n",
    "                results[index] = get_cached(cache_conn, key)\n",
    "                continue\n",
    "            prompt = [get_system_prompt()] + get_example_shots(number) + [get_final_prompt(message[\"raw_message\"])]\n",
    "            if model in GPT_MODELS:\n",
    "                model_response = ask_chatgpt(prompt, model)\n",
    "            try:\n",
    "                parsed = json.loads(model_response)\n",
    "                if parsed:\n",
    "                    results[index] = parsed\n",
    "                    set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                    cached_keys.add(key)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        with open(f\"output_premo/{run_name}\", \"w\") as jsonfile:\n",
    "            json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820261389d521ad",
   "metadata": {},
   "source": [
    "This second cell collects the data for the ollama models, for the one-shot and few-shot cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261feae0c3ae123",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "OLLAMA_MODELS = [\"mistral-nemo:12b\", \"gemma2:9b\", \"llama3.1:8b\", \"mistral-small:22b\", \"gemma2:27b\", \"deepseek-r1:8b\", \"llama3.1:70b\"]\n",
    "\n",
    "for model in OLLAMA_MODELS:\n",
    "    for number in [1, 3]:\n",
    "        run_name = model.replace(\":\", \"-\") + f\"_{str(number)}shot\" + \".json\"\n",
    "        if Path(\"output_premo/\" + run_name).exists():\n",
    "            continue\n",
    "        for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "            key = make_key(index, model, f\"{str(number)}shot\")\n",
    "            if key in cached_keys:\n",
    "                results[index] = get_cached(cache_conn, key)\n",
    "                continue\n",
    "            prompt = [get_system_prompt_with_message_and_examples(message[\"raw_message\"], number)]\n",
    "            model_response = ask_ollama(prompt, model)\n",
    "            try:\n",
    "                parsed = json.loads(model_response)\n",
    "                if parsed:\n",
    "                    results[index] = parsed\n",
    "                    set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                    cached_keys.add(key)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        with open(f\"output_premo/{run_name}\", \"w\") as jsonfile:\n",
    "            json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933279ab8c2130c",
   "metadata": {},
   "source": [
    "This third cell collects the data for the OpenAI models, for the chain of thought cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d65d2b7b3edc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "GPT_MODELS = [\"gpt-4o-mini-2024-07-18\", \"gpt-4o-2024-05-13\"]\n",
    "\n",
    "for model in GPT_MODELS:\n",
    "    run_name = model + \"_cotshot\" + \".json\"\n",
    "    if Path(\"output_premo/\"+ run_name).exists():\n",
    "        continue\n",
    "    for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "        key = make_key(index, model, \"cotshot\")\n",
    "        if key in cached_keys:\n",
    "            results[index] = get_cached(cache_conn, key)\n",
    "            continue\n",
    "        prompt = [get_system_prompt_with_message_and_examples(message[\"raw_message\"], 3, cot=True)]\n",
    "        if model in GPT_MODELS:\n",
    "            model_response = ask_chatgpt(prompt, model)\n",
    "        try:\n",
    "            parsed = json.loads(model_response)\n",
    "            if parsed:\n",
    "                results[index] = parsed\n",
    "                set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                cached_keys.add(key)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    with open(f\"output_premo/{run_name}\", \"w\") as jsonfile:\n",
    "        json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784a737c34a7e1d",
   "metadata": {},
   "source": [
    "This fourth cell collects the data for the ollama models, for the chain of thought cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1662a229d22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "OLLAMA_MODELS = [\"mistral-nemo:12b\", \"gemma2:9b\", \"llama3.1:8b\", \"mistral-small:22b\", \"gemma2:27b\", \"deepseek-r1:8b\", \"llama3.1:70b\", \"deepseek-r1:32b\"]\n",
    "\n",
    "for model in OLLAMA_MODELS:\n",
    "    run_name = model.replace(\":\", \"-\") + \"_cotshot\" + \".json\"\n",
    "    if Path(\"output_premo/\" + run_name).exists():\n",
    "        continue\n",
    "    for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "        key = make_key(index, model, \"cotshot\")\n",
    "        if key in cached_keys:\n",
    "            results[index] = get_cached(cache_conn, key)\n",
    "            continue\n",
    "        prompt = [get_system_prompt_with_message_and_examples(message[\"raw_message\"], 3, cot=True)]\n",
    "        model_response = ask_ollama(prompt, model)\n",
    "        try:\n",
    "            parsed = json.loads(model_response)\n",
    "            if parsed:\n",
    "                results[index] = parsed\n",
    "                set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                cached_keys.add(key)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    with open(f\"output_premo/{run_name}\", \"w\") as jsonfile:\n",
    "        json.dump(results, jsonfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
