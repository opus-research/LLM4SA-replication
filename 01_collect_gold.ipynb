{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917c55cbcd5f3ca5",
   "metadata": {},
   "source": [
    "Loads the libraries necessary for the script and also loads the environment variables in the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import openai\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import ollama\n",
    "import sqlite3\n",
    "import hashlib\n",
    "\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "CACHE_DB = Path(\"temp/gold_cache.db\")\n",
    "\n",
    "def init_cache(db_path=CACHE_DB):\n",
    "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path, timeout=30, isolation_level=None)\n",
    "    conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    conn.execute(\"PRAGMA synchronous=FULL;\")\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS cache (\n",
    "            key TEXT PRIMARY KEY,\n",
    "            model TEXT,\n",
    "            prompt_hash TEXT,\n",
    "            result TEXT,\n",
    "            ts INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    return conn\n",
    "\n",
    "def hash_prompt(prompt):\n",
    "    return hashlib.sha256(json.dumps(prompt, sort_keys=True, ensure_ascii=False).encode('utf-8')).hexdigest()\n",
    "\n",
    "def make_key(message_id, model, shot_label):\n",
    "    return f\"{message_id}::{model}::{shot_label}\"\n",
    "\n",
    "def get_cached(conn, key):\n",
    "    cur = conn.execute(\"SELECT result FROM cache WHERE key = ?\", (key,))\n",
    "    row = cur.fetchone()\n",
    "    return json.loads(row[0]) if row else None\n",
    "\n",
    "def set_cache(conn, key, model, prompt, result):\n",
    "    ph = hash_prompt(prompt)\n",
    "    ts = int(time.time())\n",
    "    conn.execute(\n",
    "        \"INSERT OR REPLACE INTO cache (key, model, prompt_hash, result, ts) VALUES (?, ?, ?, ?, ?)\",\n",
    "        (key, model, ph, json.dumps(result, ensure_ascii=False), ts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d40aaa7f33a876",
   "metadata": {},
   "source": [
    "Setup for utilizing the OpenAI and Ollama APIs. Also loads the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6cb4d415168c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_conn = init_cache()\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "ollama_client = ollama.Client(timeout=120)\n",
    "\n",
    "import csv\n",
    "\n",
    "dataset = []\n",
    "with open(\"data/github_gold.csv\", newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=';')\n",
    "    for row in reader:\n",
    "        if 'Text' in row and row['Text'].strip() != '':\n",
    "            dataset.append(row['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f980d1511fe436d",
   "metadata": {},
   "source": [
    "Defines helper methods for asking questions to GPT and Ollama. The ollama version contains a retry part, since, especially on windows, ollama tends to fail to response sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb380c0681f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatgpt(prompts, model):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=prompts,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "        \n",
    "def ask_ollama(prompts, model):\n",
    "    retry_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            response = ollama_client.chat(model=model, messages=prompts, format=\"json\", stream=False,\n",
    "            options={\n",
    "                \"temperature\": 0,\n",
    "                \"num_ctx\": 8192,\n",
    "                \"num_predict\": -1\n",
    "            })\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            time.sleep(5)\n",
    "            print(f\"Failed with {e}. Retrying...\")\n",
    "            retry_count += 1\n",
    "            if retry_count > 5:\n",
    "                return None\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a77bc4503a5017",
   "metadata": {},
   "source": [
    "This cell defines helper methods that generate the prompt depending on the structure utilized by the model (OpenAI and Ollama can differ) and the prompt engineering technique utilized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01ec8b38d66708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_prompt():\n",
    "    return {\"role\": \"system\",\n",
    "            \"content\": dedent(f\"\"\"\n",
    "                You are a bot that classifies messages from Github pull requests. Classify the message as one of three types of sentiment: positive, neutral, and negative.\n",
    "\n",
    "                For classification purposes, we consider love and joy (and related emotions) positive, \\\n",
    "                anger, sadness, and fear, negative, surprise can be positive or negative depending on the context, and \\\n",
    "                neutral is considered the absense of any emotions.\n",
    "\n",
    "                Return the result one of the following JSONs: {{\"sentiment\": \"positive\"}}, {{\"sentiment\": \"negative\"}} OR {{\"sentiment\": \"neutral\"}}.\n",
    "            \"\"\")}\n",
    "\n",
    "def get_system_prompt_with_message(message):\n",
    "    return {\"role\": \"user\",\n",
    "            \"content\": dedent(f\"\"\"\n",
    "                You are a bot that classifies messages from Github pull requests. Classify the message as one of three types of sentiment: positive, neutral, and negative.\n",
    "\n",
    "                For classification purposes, we consider love and joy (and related emotions) positive, \\\n",
    "                anger, sadness, and fear, negative, surprise can be positive or negative depending on the context, and \\\n",
    "                neutral is considered the absense of any emotions.\n",
    "\n",
    "                Return the result one of the following JSONs: {{\"sentiment\": \"positive\"}}, {{\"sentiment\": \"negative\"}} OR {{\"sentiment\": \"neutral\"}}.\n",
    "                \n",
    "                Message: \"{message}\"\n",
    "            \"\"\")}\n",
    "\n",
    "def get_system_prompt_with_message_and_examples(message, num_examples, cot=False):\n",
    "    return {\"role\": \"user\",\n",
    "            \"content\": dedent(f\"\"\"\n",
    "                You are a bot that classifies messages from Github pull requests. Classify the message as one of three types of sentiment: positive, neutral, and negative.\n",
    "\n",
    "                For classification purposes, we consider love and joy (and related emotions) positive, \\\n",
    "                anger, sadness, and fear, negative, surprise can be positive or negative depending on the context, and \\\n",
    "                neutral is considered the absense of any emotions.\n",
    "\n",
    "                Return the result one of the following JSONs: {{\"sentiment\": \"positive\"}}, {{\"sentiment\": \"negative\"}} OR {{\"sentiment\": \"neutral\"}}.\n",
    "                \n",
    "                Here are some examples to help you get started:\n",
    "                {get_example_strings(num_examples, cot)}\n",
    "                \n",
    "                Message: \"{message}\"\n",
    "            \"\"\")}\n",
    "\n",
    "def get_examples(num_positive, num_negative, num_neutral):\n",
    "    positive = [\n",
    "        {\n",
    "            \"message\": \"This is just to good to be true. I'm almost crying 'caus of joy. Thanks a lot, this makes life a lot easier!\",\n",
    "            \"reasoning\": \"Expresses joy and gratitude, with phrases like 'almost crying' indicating positive emotion.\",\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"Finally! Thanks for the fix!\",\n",
    "            \"reasoning\": \"Shows satisfaction and appreciation (finally, thanks), indicating a positive reaction to the change.\",\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"hehe yeah I first asked to myself, why we have to ask for defined? and not just initialize the value in the initializer. I started to delete that stuff and when I saw the initialize method I realize about the typo :).\",\n",
    "            \"reasoning\": \"Lighthearted tone (hehe, smiley) and amusement about the mistake; no anger, overall friendly/positive.\",\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    neutral = [\n",
    "        {\n",
    "            \"message\": \"This is incorrectly placed. It should be under the method signature.\",\n",
    "            \"reasoning\": \"A factual correction about code structure with no praise, anger, or other emotional language.\",\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"I'm not sure we need to do this based on checksums.  If somebody touches a file, we should probably send a notification of a change.  I think we should avoid the checksums unless there's a need.\",\n",
    "            \"reasoning\": \"Reasoned technical discussion and recommendation; uncertainty is about design choice, not an emotional reaction.\",\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"`$return` will only have the last `$child`\",\n",
    "            \"reasoning\": \"A neutral observation about program behavior/bug (what the variable contains), stated without sentiment.\",\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    negative = [\n",
    "        {\n",
    "            \"message\": \"This code again! Will this hack never tire of creating new issues.\",\n",
    "            \"reasoning\": \"Shows exasperation and blame ('hack', 'again'), expressing frustration/anger toward the code.\",\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"Ahhh yes. Brain fail, sorry!\",\n",
    "            \"reasoning\": \"Expresses self-frustration and apology (brain fail), which reflects negative affect rather than neutrality.\",\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"Hate the double colons.  Bring back the old style!\",\n",
    "            \"reasoning\": \"Explicitly conveys dislike ('hate') and a complaint about style, indicating negative sentiment.\",\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return positive[0:num_positive], neutral[0:num_neutral], negative[0:num_negative]\n",
    "\n",
    "def get_example_shots(num):\n",
    "    \n",
    "    classes = {}\n",
    "    example_shots = []\n",
    "    \n",
    "    classes['positive'], classes['neutral'], classes['negative'] = get_examples(num, num, num)\n",
    "    \n",
    "    for key, value in classes.items():\n",
    "        for message in value:\n",
    "            example_shots.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message[\"message\"],\n",
    "            })\n",
    "            example_shots.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f'{\"sentiment\": \"{key}\"}',\n",
    "            })\n",
    "        \n",
    "    return example_shots\n",
    "        \n",
    "def get_example_strings(num, cot=False):\n",
    "    message = \"\"\n",
    "    classes = {}\n",
    "    \n",
    "    classes['positive'], classes['neutral'], classes['negative'] = get_examples(num, num, num)\n",
    "    \n",
    "    i = 1\n",
    "    for key, value in classes.items():\n",
    "        for msg in value:\n",
    "            message += f'Example {i}: {msg[\"message\"]}\\nResponse for Example {i}: {{\"sentiment\": \"{key}\"}}\\n'\n",
    "            if cot:\n",
    "                message += f'Reasoning for Example {i}: {msg[\"reasoning\"]}\\n'\n",
    "            i += 1\n",
    "            \n",
    "    return message\n",
    "\n",
    "def get_final_prompt(message):\n",
    "    return {\"role\": \"user\",\n",
    "            \"content\": message,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63e5d73edfa15a",
   "metadata": {},
   "source": [
    "# Data collection for the first research question (RQ1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33fdef",
   "metadata": {},
   "source": [
    "This first cell collects data from the GPT models (zero-shot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d208d29d266eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "GPT_MODELS = [\"gpt-4o-2024-05-13\", \"gpt-4o-mini-2024-07-18\"]\n",
    "\n",
    "for model in GPT_MODELS:\n",
    "    run_name = model + f\"_0shot\" + \".json\"\n",
    "    if Path(\"output/\"+ run_name).exists():\n",
    "        continue\n",
    "    for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "        key = make_key(index, model, \"0shot\")\n",
    "        if key in cached_keys:\n",
    "            results[index] = get_cached(cache_conn, key)\n",
    "            continue\n",
    "        prompt = [get_system_prompt()] + [get_final_prompt(message)]\n",
    "        model_response = ask_chatgpt(prompt, model)\n",
    "        try:\n",
    "            parsed = json.loads(model_response)\n",
    "            if parsed:\n",
    "                results[index] = parsed\n",
    "                set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                cached_keys.add(key)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    with open(f\"output/{run_name}\", \"w\") as jsonfile:\n",
    "        json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7090ed3773afa7",
   "metadata": {},
   "source": [
    "This second cell collects the data from the local models, without providing any examples (zero-shot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b1fbff5a4feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "OLLAMA_MODELS = [\"mistral-nemo:12b\", \"gemma2:9b\", \"llama3.1:8b\", \"mistral-small:22b\", \"gemma2:27b\", \"deepseek-r1:8b\", \"deepseek-r1:32b\", \"llama3.1:70b\"]\n",
    "\n",
    "for model in OLLAMA_MODELS:\n",
    "    run_name = model.replace(\":\", \"-\") + f\"_0shot\" + \".json\"\n",
    "    if Path(\"output_gold/\" + run_name).exists():\n",
    "        continue\n",
    "    for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "        key = make_key(index, model, \"0shot\")\n",
    "        if key in cached_keys:\n",
    "            results[index] = get_cached(cache_conn, key)\n",
    "            continue\n",
    "        prompt = [get_system_prompt_with_message(message)]\n",
    "        model_response = ask_ollama(prompt, model)\n",
    "        try:\n",
    "            parsed = json.loads(model_response)\n",
    "            if parsed:\n",
    "                results[index] = parsed\n",
    "                set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                cached_keys.add(key)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    with open(f\"output_gold/{run_name}\", \"w\") as jsonfile:\n",
    "        json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c01e7f6fe2a15",
   "metadata": {},
   "source": [
    "# Data collection for the second research question (RQ2)\n",
    "\n",
    "This first cell collects the data for the OpenAI models, for the one-shot and few-shot cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91876f6e6e07a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "GPT_MODELS = [\"gpt-4o-mini-2024-07-18\", \"gpt-4o-2024-05-13\"]\n",
    "\n",
    "for model in GPT_MODELS:\n",
    "    for number in [1, 3]:\n",
    "        run_name = model + f\"_{str(number)}shot\" + \".json\"\n",
    "        if Path(\"output/\"+ run_name).exists():\n",
    "            continue\n",
    "        for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "            key = make_key(index, model, f\"{str(number)}shot\")\n",
    "            if key in cached_keys:\n",
    "                results[index] = get_cached(cache_conn, key)\n",
    "                continue\n",
    "            prompt = [get_system_prompt()] + get_example_shots(number) + [get_final_prompt(message)]\n",
    "            model_response = ask_chatgpt(prompt, model)\n",
    "            try:\n",
    "                parsed = json.loads(model_response)\n",
    "                if parsed:\n",
    "                    results[index] = parsed\n",
    "                    set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                    cached_keys.add(key)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        with open(f\"output/{run_name}\", \"w\") as jsonfile:\n",
    "            json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820261389d521ad",
   "metadata": {},
   "source": [
    "This second cell collects the data for the ollama models, for the one-shot and few-shot cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261feae0c3ae123",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "OLLAMA_MODELS = [\"mistral-nemo:12b\", \"gemma2:9b\", \"llama3.1:8b\", \"mistral-small:22b\", \"gemma2:27b\", \"deepseek-r1:8b\", \"deepseek-r1:32b\", \"llama3.1:70b\"]\n",
    "\n",
    "for model in OLLAMA_MODELS:\n",
    "    for number in [1, 3]:\n",
    "        run_name = model.replace(\":\", \"-\") + f\"_{str(number)}shot\" + \".json\"\n",
    "        if Path(\"output/\" + run_name).exists():\n",
    "            continue\n",
    "        for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "            key = make_key(index, model, f\"{str(number)}shot\")\n",
    "            if key in cached_keys:\n",
    "                results[index] = get_cached(cache_conn, key)\n",
    "                continue\n",
    "            prompt = [get_system_prompt_with_message_and_examples(message, number)]\n",
    "            model_response = ask_ollama(prompt, model)\n",
    "            try:\n",
    "                parsed = json.loads(model_response)\n",
    "                if parsed:\n",
    "                    results[index] = parsed\n",
    "                    set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                    cached_keys.add(key)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        with open(f\"output/{run_name}\", \"w\") as jsonfile:\n",
    "            json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933279ab8c2130c",
   "metadata": {},
   "source": [
    "This third cell collects the data for the OpenAI models, for the chain of thought cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d65d2b7b3edc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "GPT_MODELS = [\"gpt-4o-mini-2024-07-18\", \"gpt-4o-2024-05-13\"]\n",
    "\n",
    "for model in GPT_MODELS:\n",
    "    run_name = model + \"_cotshot\" + \".json\"\n",
    "    if Path(\"output/\"+ run_name).exists():\n",
    "        continue\n",
    "    for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "        key = make_key(index, model, \"cotshot\")\n",
    "        if key in cached_keys:\n",
    "            results[index] = get_cached(cache_conn, key)\n",
    "            continue\n",
    "        prompt = [get_system_prompt_with_message_and_examples(message, number, cot=True)]\n",
    "        model_response = ask_chatgpt(prompt, model)\n",
    "        try:\n",
    "            parsed = json.loads(model_response)\n",
    "            if parsed:\n",
    "                results[index] = parsed\n",
    "                set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                cached_keys.add(key)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    with open(f\"output/{run_name}\", \"w\") as jsonfile:\n",
    "        json.dump(results, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784a737c34a7e1d",
   "metadata": {},
   "source": [
    "This fourth cell collects the data for the ollama models, for the chain of thought cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1662a229d22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_keys = set([r[0] for r in cache_conn.execute(\"SELECT key FROM cache\")])\n",
    "\n",
    "results = [None] * len(dataset)\n",
    "\n",
    "OLLAMA_MODELS = [\"mistral-nemo:12b\", \"gemma2:9b\", \"llama3.1:8b\", \"mistral-small:22b\", \"gemma2:27b\", \"deepseek-r1:8b\", \"deepseek-r1:32b\", \"llama3.1:70b\"]\n",
    "\n",
    "for model in OLLAMA_MODELS:\n",
    "    run_name = model.replace(\":\", \"-\") + \"_cotshot\" + \".json\"\n",
    "    if Path(\"output/\" + run_name).exists():\n",
    "        continue\n",
    "    for index, message in tqdm(enumerate(dataset), desc=run_name):\n",
    "        key = make_key(index, model, \"cotshot\")\n",
    "        if key in cached_keys:\n",
    "            results[index] = get_cached(cache_conn, key)\n",
    "            continue\n",
    "        prompt = [get_system_prompt_with_message_and_examples(message, number, cot=True)]\n",
    "        model_response = ask_ollama(prompt, model)\n",
    "        try:\n",
    "            parsed = json.loads(model_response)\n",
    "            if parsed:\n",
    "                results[index] = parsed\n",
    "                set_cache(cache_conn, key, model, prompt, parsed)\n",
    "                cached_keys.add(key)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    with open(f\"output/{run_name}\", \"w\") as jsonfile:\n",
    "        json.dump(results, jsonfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
