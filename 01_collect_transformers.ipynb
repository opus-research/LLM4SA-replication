{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550ce923",
   "metadata": {},
   "source": [
    "This notebook fine-tunes and evaluates BERT, XLNet, RoBERTa, and ALBERT on both Gold and Premo datasets for sentiment analysis.\n",
    "\n",
    "It was written based on the approach used in the following paper: Zhang, T., Xu, B., Thung, F., Haryono, S. A., Lo, D., & Jiang, L. (2020, September). Sentiment analysis for software engineering: How far can pre-trained transformer models go?. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME) (pp. 70-80). IEEE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e2c86",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c180efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    XLNetTokenizer, XLNetForSequenceClassification,\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    AlbertTokenizer, AlbertForSequenceClassification\n",
    ")\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e57208",
   "metadata": {},
   "source": [
    "## Configuration and Hyperparameters\n",
    "\n",
    "Define hyperparameters and model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Model configurations\n",
    "MODELS = [\n",
    "    (BertForSequenceClassification, BertTokenizer, 'bert-base-cased', 'bert'),\n",
    "    (XLNetForSequenceClassification, XLNetTokenizer, 'xlnet-base-cased', 'xlnet'),\n",
    "    (RobertaForSequenceClassification, RobertaTokenizer, 'roberta-base', 'roberta'),\n",
    "    (AlbertForSequenceClassification, AlbertTokenizer, 'albert-base-v1', 'albert')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cfff12",
   "metadata": {},
   "source": [
    "## Setup Device and Seed\n",
    "\n",
    "Configure the device (GPU/CPU) and set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9055a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1acee",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "Functions to load the Gold dataset (CSV) and Premo dataset (JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b590b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_dataset():\n",
    "    \"\"\"Load the Gold dataset from CSV\"\"\"\n",
    "    print(\"Loading Gold dataset...\")\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(\"data/github_gold.csv\", newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=';')\n",
    "        for row in reader:\n",
    "            if 'Text' in row and row['Text'].strip() != '':\n",
    "                texts.append(row['Text'])\n",
    "                labels.append(row['Polarity'])\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} samples from Gold dataset\")\n",
    "    return texts, labels\n",
    "\n",
    "def load_premo_dataset():\n",
    "    \"\"\"Load the Premo dataset from JSON\"\"\"\n",
    "    print(\"Loading Premo dataset...\")\n",
    "    \n",
    "    with open(\"data/dataset.json\") as file:\n",
    "        dataset = json.load(file)\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for message in dataset:\n",
    "        text = message[\"raw_message\"]\n",
    "            \n",
    "        # Extract label\n",
    "        if \"part2_aggregate\" in message and \"polarity\" in message[\"part2_aggregate\"]:\n",
    "            if message[\"part2_aggregate\"][\"polarity\"] != \"undefined\":\n",
    "                label = message[\"part2_aggregate\"][\"polarity\"]\n",
    "            elif \"discussion_polarity\" in message:\n",
    "                label = message[\"discussion_polarity\"]\n",
    "            else:\n",
    "                continue\n",
    "        elif \"discussion_polarity\" in message:\n",
    "            label = message[\"discussion_polarity\"]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} samples from Premo dataset\")\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f2242",
   "metadata": {},
   "source": [
    "## Data Encoding\n",
    "\n",
    "Function to tokenize and encode text data for transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(texts, labels, tokenizer):\n",
    "    \"\"\"Encode texts and labels for transformer models\"\"\"\n",
    "    label_map = {'positive': 1, 'negative': 2, 'neutral': 0}\n",
    "    numeric_labels = [label_map[label] for label in labels]\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    print(\"Encoding data...\")\n",
    "    for text in tqdm(texts):\n",
    "        encoded_dict = tokenizer(\n",
    "            str(text),\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels_tensor = torch.tensor(numeric_labels)\n",
    "    \n",
    "    return input_ids, attention_masks, labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9dbfa",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "Function to train the model for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d86f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, optimizer):\n",
    "    print(f\"Training for {EPOCHS} epochs...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in trange(EPOCHS, desc=\"Epoch\"):\n",
    "        tr_loss = 0\n",
    "        nb_tr_steps = 0\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                          attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_steps += 1\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} - Train loss: {tr_loss/nb_tr_steps:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7255a702",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "\n",
    "Function to evaluate the model on test data and return predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    \"\"\"Evaluate the model and return predictions\"\"\"\n",
    "    print(\"Evaluating model...\")\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                          attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    \n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1)\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "    \n",
    "    return flat_predictions, flat_true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166d0c0",
   "metadata": {},
   "source": [
    "## Metrics Calculation\n",
    "\n",
    "Functions to calculate, display, and save evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8d98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_labels, predictions):\n",
    "    labels = [0, 1, 2] \n",
    "    label_names = ['neutral', 'positive', 'negative']\n",
    "    \n",
    "    precision = precision_score(true_labels, predictions, labels=labels, average=None, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, labels=labels, average=None, zero_division=0)\n",
    "    f1_scores = f1_score(true_labels, predictions, labels=labels, average=None, zero_division=0)\n",
    "    \n",
    "    macro_precision = precision_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    micro_precision = precision_score(true_labels, predictions, average='micro', zero_division=0)\n",
    "    macro_recall = recall_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    micro_recall = recall_score(true_labels, predictions, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    micro_f1 = f1_score(true_labels, predictions, average='micro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'per_class': {\n",
    "            'precision': {label_names[i]: precision[i] for i in range(3)},\n",
    "            'recall': {label_names[i]: recall[i] for i in range(3)},\n",
    "            'f1_score': {label_names[i]: f1_scores[i] for i in range(3)}\n",
    "        },\n",
    "        'macro': {\n",
    "            'precision': macro_precision,\n",
    "            'recall': macro_recall,\n",
    "            'f1_score': macro_f1\n",
    "        },\n",
    "        'micro': {\n",
    "            'precision': micro_precision,\n",
    "            'recall': micro_recall,\n",
    "            'f1_score': micro_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics, model_name, dataset_name):\n",
    "    \"\"\"Print metrics in a formatted way\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {model_name} on {dataset_name} dataset\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    print(f\"{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "    print(\"-\" * 48)\n",
    "    for label in ['neutral', 'positive', 'negative']:\n",
    "        print(f\"{label:<12} {metrics['per_class']['precision'][label]:<12.4f} \"\n",
    "              f\"{metrics['per_class']['recall'][label]:<12.4f} \"\n",
    "              f\"{metrics['per_class']['f1_score'][label]:<12.4f}\")\n",
    "    \n",
    "    print(\"\\nMacro averages:\")\n",
    "    print(f\"Precision: {metrics['macro']['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['macro']['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {metrics['macro']['f1_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\nMicro averages:\")\n",
    "    print(f\"Precision: {metrics['micro']['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['micro']['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {metrics['micro']['f1_score']:.4f}\")\n",
    "\n",
    "def save_results(metrics, predictions, true_labels, model_name, dataset_name, output_dir):\n",
    "    \"\"\"Save results to JSON file\"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'dataset': dataset_name,\n",
    "        'metrics': metrics,\n",
    "        'predictions': predictions.tolist(),\n",
    "        'true_labels': true_labels.tolist()\n",
    "    }\n",
    "    \n",
    "    output_file = output_dir / f\"{model_name}_{dataset_name}.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a95e6",
   "metadata": {},
   "source": [
    "## Complete Training Pipeline\n",
    "\n",
    "Function that orchestrates the complete training and evaluation process for a single model-dataset combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db57a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model_config, texts, labels, dataset_name, output_dir):\n",
    "    model_class, tokenizer_class, pretrained_name, model_name = model_config\n",
    "    \n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# Training {model_name.upper()} on {dataset_name} dataset\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    seed_torch(RANDOM_STATE)\n",
    "    \n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        texts, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Train size: {len(train_texts)}, Test size: {len(test_texts)}\")\n",
    "    \n",
    "    print(f\"Loading tokenizer: {pretrained_name}\")\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_name, do_lower_case=True)\n",
    "    \n",
    "    train_inputs, train_masks, train_labels_tensor = encode_data(train_texts, train_labels, tokenizer)\n",
    "    test_inputs, test_masks, test_labels_tensor = encode_data(test_texts, test_labels, tokenizer)\n",
    "    \n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels_tensor)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_labels_tensor)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print(f\"Loading model: {pretrained_name}\")\n",
    "    model = model_class.from_pretrained(pretrained_name, num_labels=3)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_model(model, train_dataloader, optimizer)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predictions, true_labels = evaluate_model(model, test_dataloader)\n",
    "    eval_time = time.time() - start_time\n",
    "    print(f\"Evaluation completed in {eval_time:.2f} seconds\")\n",
    "    \n",
    "    metrics = calculate_metrics(true_labels, predictions)\n",
    "    \n",
    "    print_metrics(metrics, model_name, dataset_name)\n",
    "    \n",
    "    save_results(metrics, predictions, true_labels, model_name, dataset_name, output_dir)\n",
    "    \n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf65b1d",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Transformer Fine-tuning and Evaluation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "output_dir = Path(\"output_transformers\")\n",
    "\n",
    "gold_texts, gold_labels = load_gold_dataset()\n",
    "premo_texts, premo_labels = load_premo_dataset()\n",
    "\n",
    "datasets = [\n",
    "    (\"gold\", gold_texts, gold_labels),\n",
    "    (\"premo\", premo_texts, premo_labels)\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINE-TUNED EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "for dataset_name, texts, labels in datasets:\n",
    "    for model_config in MODELS:\n",
    "        try:\n",
    "            train_and_evaluate(model_config, texts, labels, dataset_name, output_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError training {model_config[3]} on {dataset_name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"All experiments completed!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
